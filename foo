{"version":3,"term":{"cols":118,"rows":38,"type":"xterm-256color","version":"iTerm2 3.6.5","theme":{"fg":"#c7c7c7","bg":"#000000","palette":"#626262:#f18a78:#c3f985:#fffec9:#afd4fb:#f295f8:#d1d2fb:#f2f2f2:#8f8f8f:#f7c7c0:#defcc0:#fffeda:#c8e3fd:#f6b6fa:#e6e7fd:#ffffff"}},"timestamp":1765392841,"env":{"SHELL":"/bin/zsh"}}
[0.141, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007\u001b]1;..gle_container\u0007"]
[0.002, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.046, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b="]
[0.000, "o", "\u001b[?2004h"]
[0.407, "o", "d"]
[0.076, "o", "\bdo"]
[0.093, "o", "c"]
[0.017, "o", "k"]
[0.134, "o", "e"]
[0.078, "o", "r"]
[0.273, "o", " "]
[0.156, "o", "p"]
[0.044, "o", "s"]
[0.199, "o", "\u001b[?1l\u001b>"]
[0.000, "o", "\u001b[?2004l"]
[0.000, "o", "\r\r\n"]
[0.000, "o", "\u001b]2;docker ps\u0007\u001b]1;docker\u0007"]
[0.119, "o", "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\r\n"]
[0.009, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007\u001b]1;..gle_container\u0007"]
[0.003, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.036, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b="]
[0.000, "o", "\u001b[?2004h"]
[0.905, "o", "\r\r\nbck-i-search: _\u001b[K\u001b[A\u001b[44C"]
[0.344, "o", "conduktor get all | p\u001b[4mb\u001b[24mcopy\u001b[1B\u001b[71Db_\u001b[A\u001b[64C"]
[0.094, "o", "\u001b[21Ddocker \u001b[4mb\u001b[4mu\u001b[24mild . -t con\u001b[24mduktor_quick_start_in_a_single_container              \r\r\nbck-i-search: bu_\u001b[K\u001b[A\u001b[A\u001b[49C"]
[0.048, "o", "\u001b[1C\u001b[4mu\u001b[4mi\u001b[24m\u001b[2B\u001b[53Di_\u001b[A\u001b[A\u001b[48C"]
[0.016, "o", "\u001b[2C\u001b[4mi\u001b[4ml\u001b[24m\u001b[2B\u001b[53Dl_\u001b[A\u001b[A\u001b[47C"]
[0.099, "o", "\u001b[3C\u001b[4ml\u001b[4md\u001b[24m\u001b[2B\u001b[53Dd_\u001b[A\u001b[A\u001b[46C"]
[0.443, "o", "\u001b[24mb\u001b[24mu\u001b[24mi\u001b[24ml\u001b[24md\u001b[2B\r\u001b[K\u001b[A\u001b[A\u001b[66C"]
[0.000, "o", "\u001b[?1l\u001b>"]
[0.000, "o", "\u001b[?2004l\u001b[2B\r"]
[0.001, "o", "\u001b]2;docker build . -t conduktor_quick_start_in_a_single_container\u0007\u001b]1;docker\u0007"]
[0.438, "o", "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l"]
[0.000, "o", "\r\n\u001b[?25h"]
[0.082, "o", "\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                                          docker:desktop-linux\r\n\u001b[?25h"]
[0.100, "o", "\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (30/32)                                                                        docker:desktop-linux\r\n\u001b[34m => [internal] load build definition from Dockerfile                                                             0.0s\r\n\u001b[0m\u001b[34m => => transferring dockerfile: 2.08kB                                                                           0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-gateway:3.15.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.redpanda.com/redpandadata/redpanda:v24.1.6                               0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console:1.40.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:24.04                                                  0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-data-generator:0.9                                0.0s\r"]
[0.000, "o", "\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console-cortex:1.40.0                             0.0s\r\n\u001b[0m\u001b[34m => [internal] load .dockerignore                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 2B                                                                                  0.0s\r\n\u001b[0m\u001b[34m => [internal] load build context                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 7.91kB                                                                              0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-gateway:3.15.0                                                            0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-console-cortex:1.40.0                                                     0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-data-generator:0.9                                                        0.0s"]
[0.000, "o", "\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-console:1.40.0                                                            0.0s\r\n\u001b[0m\u001b[34m => FROM docker.redpanda.com/redpandadata/redpanda:v24.1.6                                                       0.0s\r\n\u001b[0m\u001b[34m => [stage-0  1/18] FROM docker.io/library/ubuntu:24.04                                                          0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  2/18] RUN apt-get update   && apt-get install -y --no-install-recommends ca-certificates c  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  3/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/conduktor /opt/conduktor          0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  4/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/console /opt/console              0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  5/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/java /opt/java                    0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  6/18] COPY --from=conduktor/conduktor-console:1.40.0 /var/www /var/www                      0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  7/18] COPY --from=conduktor/conduktor-gateway:3.15.0 /app /opt/gateway-app                  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  8/18] COPY --from=conduktor/conduktor-console-cortex:1.40.0 /opt/monitoring /opt/monitorin  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  9/18] COPY --from=conduktor/conduktor-data-generator:0.9 /app /opt/datagen-app              0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 10/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/redpanda /usr  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 11/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/rpk /usr/bin/  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 12/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /etc/redpanda /etc/red  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 13/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /opt/redpanda /opt/red  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 14/18] COPY redpanda.yaml /etc/redpanda/redpanda.yaml                                        0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 15/18] RUN mkdir -p /var/lib/conduktor/pg1 /var/lib/conduktor/pg2 /var/lib/conduktor/redpan  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 16/18] COPY entrypoint.sh /entrypoint.sh                                                     0.0s\r\n\u001b[0m\u001b[?25h"]
[0.197, "o", "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (31/32)                                                                        docker:desktop-linux\r\n\u001b[34m => [internal] load build definition from Dockerfile                                                             0.0s\r\n\u001b[0m\u001b[34m => => transferring dockerfile: 2.08kB                                                                           0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-gateway:3.15.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.redpanda.com/redpandadata/redpanda:v24.1.6                               0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console:1.40.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:24.04                                                  0"]
[0.000, "o", ".0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-data-generator:0.9                                0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console-cortex:1.40.0                             0.0s\r\n\u001b[0m\u001b[34m => [internal] load .dockerignore                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 2B                                                                                  0.0s\r\n\u001b[0m\u001b[34m => [internal] load build context                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 7.91kB                                                                              0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-gateway:3.15.0                                                            0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-console-cortex:1.40.0                                                     0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-data-generator:0.9                                                        0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-console:1.40.0                                                            0.0s\r\n\u001b[0m\u001b[34m => FROM docker.redpanda.com/redpandadata/redpanda:v24.1.6                                                       0.0s\r\n\u001b[0m\u001b[34m => [stage-0  1/18] FROM docker.io/library/ubuntu:24.04                                                          0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  2/18] RUN apt-get update   && apt-get install -y --no-install-recommends ca-certificates c  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  3/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/conduktor /opt/conduktor          0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  4/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/console /opt/console              0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  5/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/java /opt/java                    0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  6/18] COPY --from=conduktor/conduktor-console:1.40.0 /var/www /var/www                      0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  7/18] COPY --from=conduktor/conduktor-gateway:3.15.0 /app /opt/gateway-app                  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  8/18] COPY --from=conduktor/conduktor-console-cortex:1.40.0 /opt/monitoring /opt/monitorin  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  9/18] COPY --from=conduktor/conduktor-data-generator:0.9 /app /opt/datagen-app              0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 10/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/redpanda /usr  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 11/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/rpk /usr/bin/  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 12/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /etc/redpanda /etc/red  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 13/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /opt/redpanda /opt/red  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 14/18] COPY redpanda.yaml /etc/redpanda/redpanda.yaml                                        0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 15/18] RUN mkdir -p /var/lib/conduktor/pg1 /var/lib/conduktor/pg2 /var/lib/conduktor/redpan  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 16/18] COPY entrypoint.sh /entrypoint.sh                                                     0.0s\r\n\u001b[0m\u001b[34m => [stage-0 17/18] COPY setup_gateway.sh /opt/conduktor/setup_gateway.sh                                        0.0s\r\n\u001b[0m => [stage-0 18/18] RUN chmod +x /entrypoint.sh /opt/conduktor/setup_gateway.sh                                  0.2s\r\n\u001b[?25h"]
[0.123, "o", "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (32/32)                                                                        docker:desktop-linux\r\n\u001b[34m => [internal] load build definition from Dockerfile                                                             0.0s\r\n\u001b[0m\u001b[34m => => transferring dockerfile: 2.08kB                                                                           0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-gateway:3.15.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.redpanda.com/redpandadata/redpanda:v24.1.6                               0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console:1.40.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:24.04                                           "]
[0.000, "o", "       0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-data-generator:0.9                                0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console-cortex:1.40.0                             0.0s\r\n\u001b[0m\u001b[34m => [internal] load .dockerignore                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 2B                                                                                  0.0s\r\n\u001b[0m\u001b[34m => [internal] load build context                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 7.91kB                                                                              0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-gateway:3.15.0                                                            0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-console-cortex:1.40.0                                                     0.0s\r\n\u001b[0m\u001b[34m => FROM docker.io/conduktor/conduktor-data-generator:0.9                                                        0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-console:1.40.0                                                            0.0s\r\n\u001b[0m\u001b[34m => FROM docker.redpanda.com/redpandadata/redpanda:v24.1.6                                                       0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [stage-0  1/18] FROM docker.io/library/ubuntu:24.04                                                          0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  2/18] RUN apt-get update   && apt-get install -y --no-install-recommends ca-certificates c  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  3/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/conduktor /opt/conduktor          0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  4/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/console /opt/console              0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  5/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/java /opt/java                    0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  6/18] COPY --from=conduktor/conduktor-console:1.40.0 /var/www /var/www                      0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  7/18] COPY --from=conduktor/conduktor-gateway:3.15.0 /app /opt/gateway-app                  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  8/18] COPY --from=conduktor/conduktor-console-cortex:1.40.0 /opt/monitoring /opt/monitorin  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  9/18] COPY --from=conduktor/conduktor-data-generator:0.9 /app /opt/datagen-app              0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 10/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/redpanda /usr  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 11/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/rpk /usr/bin/  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 12/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /etc/redpanda /etc/red  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 13/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /opt/redpanda /opt/red  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 14/18] COPY redpanda.yaml /etc/redpanda/redpanda.yaml                                        0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 15/18] RUN mkdir -p /var/lib/conduktor/pg1 /var/lib/conduktor/pg2 /var/lib/conduktor/redpan  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 16/18] COPY entrypoint.sh /entrypoint.sh                                                     0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [stage-0 17/18] COPY setup_gateway.sh /opt/conduktor/setup_gateway.sh                                        0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [stage-0 18/18] RUN chmod +x /entrypoint.sh /opt/conduktor/setup_gateway.sh                                  0.3s\r\n\u001b[0m"]
[0.000, "o", "\u001b[?25h"]
[0.061, "o", "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (33/33) FINISHED                                                               docker:desktop-linux\r\n\u001b[34m => [internal] load build definition from Dockerfile                                                             0.0s\r\n\u001b[0m\u001b[34m => => transferring dockerfile: 2.08kB                                                                           0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-gateway:3.15.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.redpanda.com/redpandadata/redpanda:v24.1.6                               0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console:1.40.0                                    0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:24.04                                           "]
[0.000, "o", "       0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-data-generator:0.9                                0.0s\r\n\u001b[0m\u001b[34m => [internal] load metadata for docker.io/conduktor/conduktor-console-cortex:1.40.0                             0.0s\r\n\u001b[0m\u001b[34m => [internal] load .dockerignore                                                                                0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => => transferring context: 2B                                                                                  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [internal] load build context                                                                                0.0s\r\n\u001b[0m\u001b[34m => => transferring context: 7.91kB                                                                              0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-gateway:3.15.0                                                            0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-console-cortex:1.40.0                                                     0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-data-generator:0.9                                                        0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.io/conduktor/conduktor-console:1.40.0                                                            0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => FROM docker.redpanda.com/redpandadata/redpanda:v24.1.6                                                       0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [stage-0  1/18] FROM docker.io/library/ubuntu:24.04                                                          0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  2/18] RUN apt-get update   && apt-get install -y --no-install-recommends ca-certificates c  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0  3/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/conduktor /opt/conduktor          0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  4/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/console /opt/console              0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  5/18] COPY --from=conduktor/conduktor-console:1.40.0 /opt/java /opt/java                    0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  6/18] COPY --from=conduktor/conduktor-console:1.40.0 /var/www /var/www                      0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  7/18] COPY --from=conduktor/conduktor-gateway:3.15.0 /app /opt/gateway-app                  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  8/18] COPY --from=conduktor/conduktor-console-cortex:1.40.0 /opt/monitoring /opt/monitorin  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0  9/18] COPY --from=conduktor/conduktor-data-generator:0.9 /app /opt/datagen-app              0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 10/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/redpanda /usr  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 11/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /usr/bin/rpk /usr/bin/  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 12/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /etc/redpanda /etc/red  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 13/18] COPY --from=docker.redpanda.com/redpandadata/redpanda:v24.1.6 /opt/redpanda /opt/red  0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => CACHED [stage-0 14/18] COPY redpanda.yaml /etc/redpanda/redpanda.yaml                                        0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 15/18] RUN mkdir -p /var/lib/conduktor/pg1 /var/lib/conduktor/pg2 /var/lib/conduktor/redpan  0.0s\r\n\u001b[0m\u001b[34m => CACHED [stage-0 16/18] COPY entrypoint.sh /entrypoint.sh                                                     0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => [stage-0 17/18] COPY setup_gateway.sh /opt/conduktor/setup_gateway.sh                                        0.0s\r\n\u001b[0m\u001b[34m => [stage-0 18/18] RUN chmod +x /entrypoint.sh /opt/conduktor/setup_gateway.sh                                  0.3s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => exporting to image                                                                                           0.0s\r\n\u001b[0m\u001b[34m => => exporting layers                                                                                          0.0s\r\n\u001b[0m\u001b[34m => => writing image sha256:8c85bef440d0c8074fb815fb8b8d7da6ed0f5381bec735dc785e5cde6d24f65b                     0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[34m => => naming to docker.io/library/conduktor_quick_start_in_a_single_container                                   0.0s\r\n\u001b[0m"]
[0.000, "o", "\u001b[?25h"]
[0.000, "o", "\r\nView build details: \u001b]8;;docker-desktop://dashboard/build/desktop-linux/desktop-linux/r4if67jn7yqiuj9kk575tqnn0\u001b\\docker-desktop://dashboard/build/desktop-linux/desktop-linux/r4if67jn7yqiuj9kk575tqnn0\u001b]8;;\u001b\\\r\n"]
[0.423, "o", "\u001b[1m\r\nWhat's next:\u001b[0m\r\n    View a summary of image vulnerabilities and recommendations → \u001b[36mdocker scout quickview \u001b[0m\r\n"]
[0.004, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007"]
[0.000, "o", "\u001b]1;..gle_container\u0007"]
[0.002, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.039, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[0.876, "o", "\r\r\nbck-i-search: _\u001b[K\u001b[A\u001b[44C"]
[0.743, "o", "docker build . -t conduktor_quick_start_in_a_single_containe\u001b[4mr\u001b[24m\u001b[K\r\r\nbck-i-search: r_\u001b[K\u001b[A\u001b[15D"]
[0.096, "o", "\u001b[A\u001b[65C\u001b[4mr\u001b[4mu\u001b[24mn -d --name conduktor_quick_start_in_a_single_conta\u001b[24miner \\\u001b[1B\r  -p 8080:8080 \\\r\r\n  -p 8888:8888 \\\u001b[K\r\r\n  -p 6969:6969 \\\u001b[K\r\r\n  conduktor_quick_start_in_a_single_container\u001b[K\r\r\n\u001b[K\r\r\nbck-i-search: ru_\u001b[K\u001b[7A\u001b[49C"]
[0.052, "o", "\u001b[1C\u001b[4mu\u001b[4mn\u001b[24m\u001b[7B\u001b[53Dn_\u001b[7A\u001b[48C"]
[0.776, "o", "\u001b[24mr\u001b[24mu\u001b[24mn\u001b[7B\r\u001b[K\u001b[7A\u001b[66C"]
[0.001, "o", "\u001b[?1l\u001b>"]
[0.000, "o", "\u001b[?2004l\u001b[7B\r"]
[0.000, "o", "\u001b]2;docker run -d --name conduktor_quick_start_in_a_single_container -p 8080:8080\u0007\u001b]1;docker\u0007"]
[0.103, "o", "782eb5bdf6c68ee243994288041cd3951dddf047d4f33db8c6b71344a8494c69\r\n"]
[0.192, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007"]
[0.000, "o", "\u001b]1;..gle_container\u0007"]
[0.003, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.047, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b="]
[0.000, "o", "\u001b[?2004h"]
[1.439, "o", "d"]
[0.057, "o", "\bdo"]
[0.082, "o", "k"]
[0.051, "o", "c"]
[0.143, "o", "e"]
[0.075, "o", "r"]
[0.062, "o", " "]
[0.085, "o", "l"]
[0.149, "o", "o"]
[0.058, "o", "g"]
[0.065, "o", "s"]
[0.148, "o", " "]
[0.458, "o", "\u001b[12D"]
[0.053, "o", "           \u001b[11D"]
[0.319, "o", "d"]
[0.070, "o", "\bdo"]
[0.111, "o", "c"]
[0.094, "o", "k"]
[0.065, "o", "e"]
[0.064, "o", "r"]
[0.070, "o", " "]
[0.059, "o", "l"]
[0.117, "o", "l"]
[0.100, "o", "g"]
[0.081, "o", "s"]
[0.226, "o", "\b \b"]
[0.145, "o", "\b \b"]
[0.091, "o", "\b \b"]
[0.055, "o", "o"]
[0.032, "o", "g"]
[0.037, "o", "s"]
[0.123, "o", " "]
[0.063, "o", "-"]
[0.132, "o", "f"]
[0.141, "o", " "]
[1.543, "o", "\u001b[7m782eb5bdf6c68ee243994288041cd3951dddf047d4f3\u001b[7m3\u001b[7mdb8c6b71344a8494c69\u001b[27m\u001b[K"]
[0.181, "o", "\u001b[A\u001b[54C\u001b[27m7\u001b[27m8\u001b[27m2\u001b[27me\u001b[27mb\u001b[27m5\u001b[27mb\u001b[27md\u001b[27mf\u001b[27m6\u001b[27mc\u001b[27m6\u001b[27m8\u001b[27me\u001b[27me\u001b[27m2\u001b[27m4\u001b[27m3\u001b[27m9\u001b[27m9\u001b[27m4\u001b[27m2\u001b[27m8\u001b[27m8\u001b[27m0\u001b[27m4\u001b[27m1\u001b[27mc\u001b[27md\u001b[27m3\u001b[27m9\u001b[27m5\u001b[27m1\u001b[27md\u001b[27md\u001b[27md\u001b[27mf\u001b[27m0\u001b[27m4\u001b[27m7\u001b[27md\u001b[27m4\u001b[27mf\u001b[27m33\u001b[27md\u001b[27mb\u001b[27m8\u001b[27mc\u001b[27m6\u001b[27mb\u001b[27m7\u001b[27m1\u001b[27m3\u001b[27m4\u001b[27m4\u001b[27ma\u001b[27m8\u001b[27m4\u001b[27m9\u001b[27m4\u001b[27mc\u001b[27m6\u001b[27m9"]
[0.000, "o", "\u001b[?1l\u001b>\u001b[?2004l\r\r\n"]
[0.001, "o", "\u001b]2;docker logs -f \u0007\u001b]1;docker\u0007"]
[0.097, "o", "Initializing Postgres at /var/lib/conduktor/pg1 (port 5432)\r\nThe files belonging to this database system will be owned by user \"postgres\".\r\nThis user must also own the server process.\r\n\r\nThe database cluster will be initialized with locale \"C\".\r\nThe default database encoding has accordingly been set to \"SQL_ASCII\".\r\nThe default text search configuration will be set to \"english\".\r\n\r\nData page checksums are disabled.\r\n"]
[0.001, "o", "\r\nfixing permissions on existing directory /var/lib/conduktor/pg1 ... ok\r\ncreating subdirectories ... ok\r\nselecting dynamic shared memory implementation ... posix\r\nselecting default max_connections ... 100\r\nselecting default shared_buffers ... 128MB\r\nselecting default time zone ... Etc/UTC\r\ncreating configuration files ... ok\r\nrunning bootstrap script ... ok\r\nperforming post-bootstrap initialization ... ok\r\nsyncing data to disk ... ok\r\n"]
[0.000, "o", "\r\n\r\nSuccess. You can now start the database server using:\r\n\r\n    /usr/lib/postgresql/16/bin/pg_ctl -D /var/lib/conduktor/pg1 -l logfile start\r\n\r\ninitdb: warning: enabling \"trust\" authentication for local connections\r\ninitdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.\r\n"]
[0.000, "o", "Initializing Postgres at /var/lib/conduktor/pg2 (port 5433)\r\nThe files belonging to this database system will be owned by user \"postgres\".\r\nThis user must also own the server process.\r\n\r\nThe database cluster will be initialized with locale \"C\".\r\nThe default database encoding has accordingly been set to \"SQL_ASCII\".\r\nThe default text search configuration will be set to \"english\".\r\n\r\nData page checksums are disabled.\r\n\r\nfixing permissions on existing directory /var/lib/conduktor/pg2 ... ok\r\ncreating subdirectories ... ok\r\nselecting dynamic shared memory implementation ... posix\r\nselecting default max_connections ... 100\r\nselecting default shared_buffers ... 128MB\r\nselecting default time zone ... Etc/UTC\r\ncreating configuration files ... ok\r\nrunning bootstrap script ... ok\r\nperforming post-bootstrap initialization ... ok\r\ninitdb: warning: enabling \"trust\" authentication for local connections\r\ninitdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the nex"]
[0.000, "o", "t time you run initdb.\r\n"]
[0.000, "o", "syncing data to disk ... ok\r\n\r\n\r\nSuccess. You can now start the database server using:\r\n\r\n    /usr/lib/postgresql/16/bin/pg_ctl -D /var/lib/conduktor/pg2 -l logfile start\r\n\r\nlocalhost:5432 - no response\r\nlocalhost:5432 - accepting connections\r\nlocalhost:5433 - accepting connections\r\nDO\r\n"]
[0.000, "o", "CREATE DATABASE\r\nALTER DATABASE\r\nDO\r\nCREATE DATABASE\r\nALTER DATABASE\r\n/entrypoint.sh: line 59: /proc/sys/fs/aio-max-nr: Read-only file system\r\nAll services started.\r\nRunning setup script in background...\r\nTailing logs...\r\n==> /var/log/conduktor/console.log <==\r\n\r\n==> /var/log/conduktor/datagen.log <==\r\n\r\n==> /var/log/conduktor/gateway.log <==\r\n\r\n==> /var/log/conduktor/pg1.log <==\r\n2025-12-10 18:54:12.475 UTC [47] LOG:  starting PostgreSQL 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1) on aarch64-unknown-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\r\n2025-12-10 18:54:12.476 UTC [47] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\r\n2025-12-10 18:54:12.476 UTC [47] LOG:  listening on IPv6 address \"::\", port 5432\r\n2025-12-10 18:54:12.477 UTC [47] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\r\n2025-12-10 18:54:12.480 UTC [70] LOG:  database system was shut down at 2025-12-10 18:54:10 UTC\r\n2025-12-10 18:54:12.484 UTC [47] LOG:  database system is ready to accept connectio"]
[0.000, "o", "ns\r\n2025-12-10 18:54:13.443 UTC [75] FATAL:  role \"root\" does not exist\r\n\r\n==> /var/log/conduktor/pg2.log <==\r\n2025-12-10 18:54:12.468 UTC [48] LOG:  starting PostgreSQL 16.11 (Ubuntu 16.11-0ubuntu0.24.04.1) on aarch64-unknown-linux-gnu, compiled by gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, 64-bit\r\n2025-12-10 18:54:12.468 UTC [48] LOG:  listening on IPv4 address \"0.0.0.0\", port 5433\r\n2025-12-10 18:54:12.468 UTC [48] LOG:  listening on IPv6 address \"::\", port 5433\r\n2025-12-10 18:54:12.470 UTC [48] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5433\"\r\n2025-12-10 18:54:12.473 UTC [64] LOG:  database system was shut down at 2025-12-10 18:54:11 UTC\r\n"]
[0.001, "o", "2025-12-10 18:54:12.478 UTC [48] LOG:  database system is ready to accept connections\r\n2025-12-10 18:54:13.448 UTC [77] FATAL:  role \"root\" does not exist\r\n\r\n==> /var/log/conduktor/redpanda.log <==\r\n\r\n==> /var/log/conduktor/console.log <==\r\n\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - \r\n      Welcome to Conduktor Console !\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⠟⢻⡇⠀⠀⠀⠀⣠⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡿⠋⠀⢸⣧⣤⣀⡀⠺⢿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⡿⠀⠀⠀⢸⣿⣿⣿⣿⣆⠀⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⠇⠀⠀⢀"]
[0.000, "o", "⣼⣿⣿⣿⣿⣿⣷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⡿⠀⠀⠴⠿⣿⣿⣦⣄⣠⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⡇⠀⠀⠀⠀⠀⠈⠉⠉⠛⠛⠿⢿⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⢀⡀⠀⠀⠀⠀⢿⣿⣦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣿⣿⣷⣤⣀⠀⠀⠀⠀⠐⣿⣿⣷⣦⣤⣀⣤⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠚⠛⠛⠛⠛⠛⠛⠂⠀⠀⠀⠘⢿⣿⣿⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣻⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢦⣤⣀⡀⠀⠀⢀⣤⣾⣿⠟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n"]
[0.001, "o", "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣿⣿⣾⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\r\n\r\nAll rights reserved to Conduktor Inc. https://www.conduktor.io\r\nToS : https://www.conduktor.io/terms-of-service\r\nPrivacy Policy : https://www.conduktor.io/privacy-policy\r\nChangelog : https://www.conduktor.io/changelog\r\nRoadmap : https://product.conduktor.help\r\nSupport : https://www.conduktor.io/contact/support\r\n\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Platform log level set to INFO\r\n\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Conduktor Platform version: unknown(unknown)\r\n\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Running user: uid=0(root) gid=0(root) groups=0(root)\r\n\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Init volume directories if ne"]
[0.000, "o", "eded\r\n"]
[0.000, "o", "\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Setup Conduktor Platform\r\n"]
[0.000, "o", "2025-12-10T18:54:13.643+0000 [platform-setup] \u001b[0;32mINFO\u001b[0m platform_entrypoint - Startup Conduktor platform with configuration file \"/opt/conduktor/default-platform-config.yaml\"\r\n"]
[0.000, "o", "2025-12-10T18:54:13.644+0000 [platform-setup] \u001b[0;32mINFO\u001b[0m platform_entrypoint - Read input configuration : \"/opt/conduktor/default-platform-config.yaml\"\r\n2025-12-10T18:54:13.683+0000 [platform-setup] \u001b[0;33mWARN\u001b[0m platform_cli::system - Could not determine the amount of available RAM: Limit in bytes is not set\r\n2025-12-10T18:54:13.685+0000 [platform-setup] \u001b[0;32mINFO\u001b[0m platform_entrypoint - Validate configuration\r\n2025-12-10T18:54:13.685+0000 [platform-setup] \u001b[0;32mINFO\u001b[0m platform_entrypoint - Input configuration is valid !\r\n"]
[0.000, "o", "2025-12-10T18:54:13.685+0000 [platform-setup] \u001b[0;32mINFO\u001b[0m platform_entrypoint - Create modules configurations\r\n\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Direct run mode enabled\r\n"]
[0.000, "o", "\u001b[38;5;5m2025-12-10T18:54:13Z \u001b[38;5;6m[entrypoint] \u001b[0m\u001b[38;5;2mINFO\u001b[0m - Start Console\r\n\r\n==> /var/log/conduktor/datagen.log <==\r\n18:54:14.137 [\u001b[34mmain\u001b[0;39m] \u001b[34mINFO \u001b[0;39m \u001b[36morg.example.Main\u001b[0;39m - Starting Data Generator...\r\n"]
[0.000, "o", "log4j:WARN No appenders could be found for logger (org.apache.http.client.protocol.RequestAddCookies).\r\n"]
[0.000, "o", "log4j:WARN Please initialize the log4j system properly.\r\n"]
[0.000, "o", "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\r\n"]
[0.000, "o", "18:54:14.531 [\u001b[1;31mmain\u001b[0;39m] \u001b[1;31mERROR\u001b[0;39m \u001b[36mo.example.gateway.InterceptorService\u001b[0;39m - Error during gateway health check\r\norg.apache.http.conn.HttpHostConnectException: Connect to localhost:8888 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused\r\n\tat org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:156)\r\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)\r\n"]
[0.000, "o", "\tat org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\r\n"]
[0.000, "o", "\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)\r\n\tat org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108)\r\n\tat org.example.gateway.InterceptorService.performHealthCheck(InterceptorService.java:60)\r\n"]
[0.000, "o", "\tat org.example.Service.performHealthCheckWithRetry(Service.java:13)\r\n\tat org.example.Main.main(Main.java:18)\r\nCaused by: java.net.ConnectException: Connection refused\r\n\tat java.base/sun.nio.ch.Net.connect0(Native Method)\r\n\tat java.base/sun.nio.ch.Net.connect(Net.java:589)\r\n"]
[0.000, "o", "\tat java.base/sun.nio.ch.Net.connect(Net.java:578)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:583)\r\n"]
[0.000, "o", "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\r\n"]
[0.000, "o", "\tat java.base/java.net.Socket.connect(Socket.java:751)\r\n\tat org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:75)\r\n\tat org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)\r\n\t... 12 common frames omitted\r\n"]
[0.000, "o", "18:54:14.535 [\u001b[31mmain\u001b[0;39m] \u001b[31mWARN \u001b[0;39m \u001b[36morg.example.Service\u001b[0;39m - Health check failed for Gateway Service. Retrying...\r\n"]
[0.000, "o", "\r\n==> /var/log/conduktor/redpanda.log <==\r\n\r\n\r\n"]
[0.000, "o", "Welcome to the Redpanda community!\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "Documentation: https://docs.redpanda.com - Product documentation site\r\n"]
[0.000, "o", "GitHub Discussion: https://github.com/redpanda-data/redpanda/discussions - Longer, more involved discussions\r\n"]
[0.000, "o", "GitHub Issues: https://github.com/redpanda-data/redpanda/issues - Report and track issues with the codebase\r\n"]
[0.000, "o", "Support: https://support.redpanda.com - Contact the support team privately\r\n"]
[0.000, "o", "Product Feedback: https://redpanda.com/feedback - Let us know how we can improve your experience\r\n"]
[0.000, "o", "Slack: https://redpanda.com/slack - Chat about all things Redpanda. Join the conversation!\r\n"]
[0.000, "o", "Twitter: https://twitter.com/redpandadata - All the latest Redpanda news!\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "\r\nINFO  2025-12-10 18:54:13,855 seastar - Reactor backend: linux-aio\r\n"]
[0.000, "o", "WARN  2025-12-10 18:54:13,856 [shard 0:n/a ] seastar - Creation of perf_event based stall detector failed: falling back to posix timer: std::__1::system_error (error system:1, perf_event_open() failed: Operation not permitted)\r\n"]
[0.000, "o", "WARN  2025-12-10 18:54:13,856 [shard 0:n/a ] cpu_profiler - Creation of perf_event based cpu profiler failed: falling back to posix timer: perf_event_open() failed: Operation not permitted\r\nINFO  2025-12-10 18:54:13,858 [shard 0:main] seastar - Created fair group io-queue-0 for 1 queues, capacity rate 2147483:2147483, limit 12582912, rate 16777216 (factor 1), threshold 2000, per tick grab 12582912\r\nINFO  2025-12-10 18:54:13,858 [shard 0:main] seastar - IO queue uses 0.75ms latency goal for device 0\r\nINFO  2025-12-10 18:54:13,858 [shard 0:main] seastar - Created io group dev(0), length limit 4194304:4194304, rate 2147483647:2147483647\r\nINFO  2025-12-10 18:54:13,858 [shard 0:main] seastar - Created io queue dev(0) capacities: 512:2000:2000 1024:3000:3000 2048:5000:5000 4096:9000:9000 8192:17000:17000 16384:33000:33000 32768:65000:65000 65536:129000:129000 131072:257000:257000\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,859 [shard 0:main] main - application.cc:424 - Redpanda v24.1.6 - 5e880f6fd1a610d0991b00e32c012a03b14888ca\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,859 [shard 0:main] main - application.cc:432 - kernel=6.12.54-linuxkit, nodename=782eb5bdf6c6, machine=aarch64\r\nINFO  2025-12-10 18:54:13,859 [shard 0:main] main - application.cc:360 - System resources: { cpus: 1, available memory: 1024.000MiB, reserved memory: 1.500GiB}\r\nINFO  2025-12-10 18:54:13,859 [shard 0:main] main - application.cc:368 - File handle limit: 1048576/1048576\r\nINFO  2025-12-10 18:54:13,883 [shard 0:main] cluster - config_manager.cc:510 - Can't load config cache: std::__1::__fs::filesystem::filesystem_error (error system:2, filesystem error: open failed: No such file or directory [\"/var/lib/conduktor/redpanda/config_cache.yaml\"])\r\nINFO  2025-12-10 18:54:13,883 [shard 0:main] cluster - config_manager.cc:450 - Can't load config bootstrap file: std::__1::__fs::filesystem::filesystem_error (error system:2, filesystem error: open failed: No such file or directory [\"/etc/redpanda/.bootstrap.yaml\"])\r\nINFO  2025-12-10 18:54:13,884 [shard 0:main] main - application.cc:810 - "]
[0.000, "o", "Cluster configuration properties:\r\nINFO  2025-12-10 18:54:13,884 [shard 0:main] main - application.cc:811 - (use `rpk cluster config edit` to change)\r\nINFO  2025-12-10 18:54:13,886 [shard 0:main] main - application.cc:772 - redpanda.abort_index_segment_size:50000\t- Capacity (in number of txns) of an abort index segment\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.abort_timed_out_transactions_interval_ms:10000\t- How often look for the inactive transactions and abort them\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.admin_api_require_auth:0\t- Whether admin API clients must provide HTTP Basic authentication headers\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.aggregate_metrics:0\t- Enable aggregations of metrics returned by the prometheus '/metrics' endpoint. Metric aggregation is performed by summing the values of samples by labels. Aggregations are performed where it makes sense by the shard and/or "]
[0.000, "o", "partition labels.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.alive_timeout_ms:5000\t- Time from the last node status heartbeat after which a node will be considered offline and not alive\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.alter_topic_cfg_timeout_ms:5000\t- Time to wait for entries replication in controller log when executing alter configuration request\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.append_chunk_size:16384\t- Size of direct write operations to disk in bytes\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_client_max_buffer_size:16777216\t- Maximum number of bytes the internal audit client will allocate for audit log records. Disable and re-enable auditing for changes to take affect\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_enabled:0\t- Enable/Disable audit logging.\r\nINFO  2025-12-10 18:54:13"]
[0.000, "o", ",887 [shard 0:main] main - application.cc:772 - redpanda.audit_enabled_event_types:{management, authenticate, admin}\t- List of event classes that will be audited, options are: [management, produce, consume, describe, heartbeat, authenticate, admin, schema_registry]. Please refer to the documentation to know exactly which request(s) map to a particular audit event type.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_excluded_principals:{}\t- List of user principals to exclude from auditing\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_excluded_topics:{}\t- List of topics to exclude from auditing\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_log_num_partitions:12\t- Number of partitions for the internal audit log topic. Attempt to create topic is only performed if it doesn't already exist, disable and re-enable auditing for changes to take affect\r\nINFO  2025-12-10 18:54:13,887 [shard 0:ma"]
[0.000, "o", "in] main - application.cc:772 - redpanda.audit_log_replication_factor:{nullopt}\t- Replication factor of the internal audit log topic. Attempt to create topic is only performed if it doesn't already exist, disable and re-enable auditing for changes to take affect.  If unset, defaults to `default_topic_replication`\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_queue_drain_interval_ms:500\t- Frequency in which per shard audit logs are batched to client for write to audit log. Longer intervals allow for greater change for coalescing duplicates (great for high throughput auditing scenarios) but increase the risk of data loss during hard shutdowns.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.audit_queue_max_buffer_size_per_shard:1048576\t- Maximum amount of memory allowed in the audit buffer per shard Once this value is reached, any request handlers that cannot enqueue audit messages will return a non retryable error to the client. Note"]
[0.000, "o", " that this only will occur when handling requests that are currently enabled for auditing.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.auto_create_topics_enabled:0\t- Allow topic auto creation\r\n"]
[0.007, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_access_key:{nullopt}\t- AWS access key\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_api_endpoint:{nullopt}\t- Optional API endpoint\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_api_endpoint_port:443\t- TLS port override\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_attempt_cluster_restore_on_bootstrap:0\t- If set to `true`, when a cluster is started for the first time and there is cluster metadata in the configured cloud storage bucket, Redpanda automatically starts a cluster restore from that metadata. If using an automated method for deployment where it's not easy to predictably determine that a restore is needed, we recommend setting to `true`. Take care to ensure that in such deployments, a cluster bootstrap with a given bucket means that any previous cluster usi"]
[0.000, "o", "ng that bucket is fully destroyed; otherwise tiered storage subsystems may interfere with each other.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_adls_endpoint:{nullopt}\t- Azure Data Lake Storage v2 endpoint override. Use when Hierarchical Namespaces are enabled on your storage account and you have set up a custom endpoint.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_adls_port:{nullopt}\t- Azure Data Lake Storage v2 port override. Also see cloud_storage_azure_adls_endpoint.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_container:{nullopt}\t- The name of the Azure container to use with Tiered Storage. Note that the container must belong to 'cloud_storage_azure_storage_account'\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_hierarchical_namespace_enabled:{nullopt}\t- Whether or not Azure"]
[0.000, "o", " Hierarchical Namespaces are enabled on the cloud_storage_azure_storage_account. If this property is not set, cloud_storage_azure_shared_key must be set, and each node will try to determine at startup if HNS is enabled. Setting this property to True will disable the check and assume HNS is active. Setting to False will disable the check and assume that HNS is not active.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_managed_identity_id:{nullopt}\t- The managed identity ID to use with Azure Managed Identities. This takes affect when the cloud_storage_credential_source configuration option is set to azure_vm_instance_metadata.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_shared_key:{nullopt}\t- The shared key to be used for Azure Shared Key authentication with the configured Azure storage account (see 'cloud_storage_azure_storage_account)'. Note that Redpanda expects this string to be Base64 encoded."]
[0.000, "o", "\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_azure_storage_account:{nullopt}\t- The name of the Azure storage account to use with Tiered Storage\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_backend:unknown\t- Optional cloud storage backend variant used to select API capabilities. If not supplied, will be inferred from other configuration parameters.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_background_jobs_quota:5000\t- The number of total requests that the cloud storage background jobs are allowed to make during one background housekeeping run. This is a per shard limit.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_bucket:{nullopt}\t- AWS bucket that should be used to store data\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_check_interval:5000\t- Mi"]
[0.000, "o", "nimum time between trims of tiered storage cache.  If a fetch operation requires trimming the cache, and the most recent trim was within this period, then trimming will be delayed until this period has elapsed\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_chunk_size:16777216\t- Size of chunks of segments downloaded into cloud storage cache. Reduces space usage by only downloading the necessary chunk from a segment.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_max_objects:100000\t- Maximum number of objects that may be held in the tiered storage cache.  This applies simultaneously with `cloud_storage_cache_size`, and which ever limit is hit first will drive trimming of the cache.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_size:0\t- Max size of archival cache\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_size_percent:{20}\t- The maximum size of the archival cache as a percentage of unreserved disk space (see disk_reservation_percent). The default value for this option is tuned for a shared disk configuration. When using a dedicated cache disk consider increasing the value.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_trim_carryover_bytes:262144\t- The cache performs a recursive directory inspection during the cache trim. The information obtained during the inspection can be carried over to the next trim operation. This parameter sets a limit on the memory occupied by objects that can be carried over from one trim to next, and allows cache to quickly unblock readers before starting the directory inspection.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_chunk_eviction_strategy:eager\t- Selects a strategy fo"]
[0.000, "o", "r evicting unused cache chunks.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_chunk_prefetch:0\t- Number of chunks to prefetch ahead of every downloaded chunk\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cluster_metadata_num_consumer_groups_per_upload:1000\t- Number of groups to upload in a single snapshot object during consumer offsets upload. Setting a lower value will mean a larger number of smaller snapshots are uploaded.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cluster_metadata_retries:5\t- Number of attempts metadata operations may be retried.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cluster_metadata_upload_interval_ms:3600000\t- Time interval to wait between cluster metadata uploads.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cluster_metadata"]
[0.000, "o", "_upload_timeout_ms:60000\t- Timeout for cluster metadata uploads.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_credentials_host:{nullopt}\t- The hostname to connect to for retrieving role based credentials. Derived from cloud_storage_credentials_source if not set. Only required when using IAM role based access.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_credentials_source:config_file\t- The source of credentials to connect to cloud services\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_disable_archiver_manager:1\t- Use legacy upload mode and do not start archiver_manager.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_disable_chunk_reads:0\t- Disable chunk reads and switch back to legacy mode where full segments are downloaded.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.c"]
[0.000, "o", "loud_storage_disable_metadata_consistency_checks:1\t- Disable all metadata consistency checks. This will allow redpanda to replay logs with inconsistent tiered-storage metadata. Normally, this option should be disabled.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_disable_read_replica_loop_for_tests:0\t- Begins the read replica sync loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_disable_tls:0\t- Disable TLS for all S3 connections\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_disable_upload_consistency_checks:0\t- Disable all upload consistency checks. This will allow redpanda to upload logs with gaps and replicate metadata with consistency violations. Normally, this options should be disabled.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] m"]
[0.001, "o", "ain - application.cc:772 - redpanda.cloud_storage_disable_upload_loop_for_tests:0\t- Begins the upload loop in tiered-storage-enabled topic partitions. The property exists to simplify testing and shouldn't be set in production.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_enable_compacted_topic_reupload:1\t- Enable re-uploading data for compacted topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_enable_remote_read:0\t- Default remote read config value for new topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_enable_remote_write:0\t- Default remote write value for new topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_enable_scrubbing:0\t- Enable scrubbing of cloud storage partitions. The scrubber validates the integrity of data and metadata uploaded to cloud storage\r\nINFO  2025-12-10 18:54:13,887 [shard 0"]
[0.000, "o", ":main] main - application.cc:772 - redpanda.cloud_storage_enable_segment_merging:1\t- Enables adjacent segment merging. The segments are reuploaded if there is an opportunity for that and if it will improve the tiered-storage performance\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_enabled:0\t- Enable archival storage\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_full_scrub_interval_ms:43200000\t- Time interval between a final scrub and thte next scrub\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_garbage_collect_timeout_ms:30000\t- Timeout for running the cloud storage garbage collection (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_graceful_transfer_timeout_ms:{5000}\t- Time limit on waiting for uploads to complete before a leadership transfer.  If this is null, leadership transfers will proceed without wa"]
[0.000, "o", "iting.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_housekeeping_interval_ms:300000\t- Interval for cloud storage housekeeping tasks\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_hydrated_chunks_per_segment_ratio:0.7\t- The maximum number of chunks per segment that can be hydrated at a time. Above this number, unused chunks will be trimmed.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_hydration_timeout_ms:600000\t- Duration to wait for a hydration request to be fulfilled, if hydration is not completed within this time, the consumer will be notified with a timeout error.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_idle_threshold_rps:10\t- The cloud storage request rate threshold for idle state detection. If the average request rate for the configured period is lower than this threshold the cloud storage is con"]
[0.000, "o", "sidered being idle.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_idle_timeout_ms:10000\t- Timeout used to detect idle state of the cloud storage API. If the average cloud storage request rate is below this threshold for a configured amount of time the cloud storage is considered idle and the housekeeping jobs are started.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_initial_backoff_ms:100\t- Initial backoff time for exponential backoff algorithm (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_manifest_cache_size:1048576\t- Amount of memory that can be used to handle tiered-storage metadata\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_manifest_max_upload_interval_sec:{60000}\t- Wait at least this long between partition manifest uploads. Actual time between uploads may be greater than this interval. If this i"]
[0.000, "o", "s null, metadata will be updated after each segment upload.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_manifest_upload_timeout_ms:10000\t- Manifest upload timeout (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_materialized_manifest_ttl_ms:10000\t- The time interval that determines how long the materialized manifest can stay in cache under contention. This parameter is used for performance tuning. When the spillover manifest is materialized and stored in cache and the cache needs to evict it it will use 'cloud_storage_materialized_manifest_ttl_ms' value as a timeout. The cursor that uses the spillover manifest uses this value as a TTL interval after which it stops referencing the manifest making it available for eviction. This only affects spillover manifests under contention.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_concurrent_hydrations_per_sha"]
[0.000, "o", "rd:{nullopt}\t- Maximum concurrent segment hydrations of remote data per CPU core.  If unset, value of `cloud_storage_max_connections / 2` is used, which means that half of available S3 bandwidth could be used to download data from S3. If the cloud storage cache is empty every new segment reader will require a download. This will lead to 1:1 mapping between number of partitions scanned by the fetch request and number of parallel downloads. If this value is too large the downloads can affect other workloads. In case of any problem caused by the tiered-storage reads this value can be lowered. This will only affect segment hydrations (downloads) but won't affect cached segments. If fetch request is reading from the tiered-storage cache its concurrency will only be limited by available memory.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_connection_idle_time_ms:5000\t- Max https connection idle time (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - app"]
[0.000, "o", "lication.cc:772 - redpanda.cloud_storage_max_connections:20\t- Max number of simultaneous connections to S3 per shard (includes connections used for both uploads and downloads)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_materialized_segments_per_shard:{nullopt}\t- Maximum concurrent readers of remote data per CPU core.  If unset, value of `topic_partitions_per_shard` multiplied by 2 is used.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_partition_readers_per_shard:{nullopt}\t- Maximum partition readers per shard (deprecated)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_segment_readers_per_shard:{nullopt}\t- Maximum concurrent I/O cursors of materialized remote segments per CPU core.  If unset, value of `topic_partitions_per_shard` is used, i.e. one segment reader per partition if the shard is at its maximum partition capacity.  These readers are c"]
[0.000, "o", "achedacross Kafka consume requests and store a readahead buffer.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_segments_pending_deletion_per_partition:5000\t- The per-partition limit for the number of segments pending deletion from the cloud. Segments can be deleted due to retention or compaction. If this limit is breached and deletion fails, then segments will be orphaned in the cloud and will have to be removed manually\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_max_throughput_per_shard:{1073741824}\t- Max throughput used by tiered-storage per shard in bytes per second. This value is an upper bound of the throughput available to the tiered-storage subsystem. This parameter is intended to be used as a safeguard and in tests when we need to set precise throughput value independent of actual storage media. Please use 'cloud_storage_throughput_limit_percent' instead of this parameter in the production envi"]
[0.000, "o", "ronment.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_metadata_sync_timeout_ms:10000\t- Timeout for SI metadata synchronization\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_min_chunks_per_segment_threshold:5\t- The minimum number of chunks per segment for trimming to be enabled. If the number of chunks in a segment is below this threshold, the segment is small enough that all chunks in it can be hydrated at any given time\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_partial_scrub_interval_ms:3600000\t- Time interval between two partial scrubs of the same partition\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_readreplica_manifest_sync_timeout_ms:30000\t- Timeout to check if new data is available for partition in S3 for read replica\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_"]
[0.000, "o", "storage_reconciliation_interval_ms:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_recovery_temporary_retention_bytes_default:1073741824\t- Retention in bytes for topics created during automated recovery\r\n"]
[0.005, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_recovery_topic_validation_depth:10\t- Number of segment metadata to validate, from newest to oldest, when `cloud_storage_recovery_topic_validation_mode` is `check_manifest_and_segment_metadata`\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_recovery_topic_validation_mode:check_manifest_existence\t- Validation mode performed before recovering a topic from cloud storage\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_region:{nullopt}\t- AWS region that houses the bucket used for storage\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_roles_operation_timeout_ms:30000\t- Timeout for IAM role related operations (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_scrubbing_interval_jitter_ms:600000\t- Jitter applied to the cloud st"]
[0.000, "o", "orage scrubbing interval.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_secret_key:{nullopt}\t- AWS secret key\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_segment_max_upload_interval_sec:{3600000}\t- Time that segment can be kept locally without uploading it to the remote storage (sec)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_segment_size_min:{nullopt}\t- Smallest acceptable segment size in the cloud storage. Default: cloud_storage_segment_size_target/2\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_segment_size_target:{nullopt}\t- Desired segment size in the cloud storage. Default: segment.bytes\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_segment_upload_timeout_ms:30000\t- Log segment upload timeout (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main "]
[0.000, "o", "- application.cc:772 - redpanda.cloud_storage_spillover_manifest_max_segments:{nullopt}\t- Maximum number of elements in the spillover manifest that can be offloaded to the cloud storage. This property is similar to 'cloud_storage_spillover_manifest_size' but it triggers spillover based on number of segments instead of the size of the manifest in bytes. The property exists to simplify testing and shouldn't be set in the production environment\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_spillover_manifest_size:{65536}\t- The size of the manifest which can be offloaded to the cloud. If the size of the local manifest stored in redpanda exceeds cloud_storage_spillover_manifest_size x2 the spillover mechanism will split the manifest into two parts and one of them will be uploaded to S3.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_throughput_limit_percent:{50}\t- Max throughput used by tiered-storage per node expr"]
[0.000, "o", "essed as a percentage of the disk bandwidth. If the server has several disks Redpanda will take into account only the one which is used to store tiered-storage cache. Note that even if the tiered-storage is allowed to use full bandwidth of the disk (100%) it won't necessary use it in full. The actual usage depend on your workload and the state of the tiered-storage cache. This parameter is a safeguard that prevents tiered-storage from using too many system resources and not a performance tuning knob.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_topic_purge_grace_period_ms:30000\t- Grace period during which the purger will refuse to purge the topic.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_trust_file:{nullopt}\t- Path to certificate that should be used to validate server certificate during TLS handshake\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_upload_"]
[0.000, "o", "ctrl_d_coeff:0\t- derivative coefficient for upload PID controller.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_upload_ctrl_max_shares:1000\t- maximum number of IO and CPU shares that archival upload can use\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_upload_ctrl_min_shares:100\t- minimum number of IO and CPU shares that archival upload can use\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_upload_ctrl_p_coeff:-2\t- proportional coefficient for upload PID controller\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_upload_ctrl_update_interval_ms:60000\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_upload_loop_initial_backoff_ms:100\t- Initial backoff interval when there is nothing to upload for a partition (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main -"]
[0.000, "o", " application.cc:772 - redpanda.cloud_storage_upload_loop_max_backoff_ms:10000\t- Max backoff interval when there is nothing to upload for a partition (ms)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cluster_id:{nullopt}\t- Cluster identifier\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compacted_log_segment_size:268435456\t- How large in bytes should each compacted log segment be (default 256MiB)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_backlog_size:{nullopt}\t- target backlog size for compaction controller. if not set compaction target compaction backlog would be equal to \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_d_coeff:0.2\t- derivative coefficient for compaction PID controller.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_i_coeff:0\t- integral coefficient for compactio"]
[0.001, "o", "n PID controller.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_max_shares:1000\t- maximum number of IO and CPU shares that compaction process can use\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_min_shares:10\t- minimum number of IO and CPU shares that compaction process can use\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_p_coeff:-12.5\t- proportional coefficient for compaction PID controller. This has to be negative since compaction backlog should decrease when number of compaction shares increases\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.compaction_ctrl_update_interval_ms:30000\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.controller_backend_housekeeping_interval_ms:1000\t- Interval between iterations of controller backend housekeeping loop\r\nINFO  2025-12-10 18:54:13,887"]
[0.000, "o", " [shard 0:main] main - application.cc:772 - redpanda.controller_log_accummulation_rps_capacity_acls_and_users_operations:{nullopt}\t- Maximum capacity of rate limit accumulationin controller acls and users operations limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.controller_log_accummulation_rps_capacity_configuration_operations:{nullopt}\t- Maximum capacity of rate limit accumulationin controller configuration operations limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.controller_log_accummulation_rps_capacity_move_operations:{nullopt}\t- Maximum capacity of rate limit accumulationin controller move operations limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.controller_log_accummulation_rps_capacity_node_management_operations:{nullopt}\t- Maximum capacity of rate limit accumulationin controller node management operations limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.c"]
[0.000, "o", "c:772 - redpanda.controller_log_accummulation_rps_capacity_topic_operations:{nullopt}\t- Maximum capacity of rate limit accumulationin controller topic operations limit\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.controller_snapshot_max_age_sec:60000\t- Max time that will pass before we make an attempt to create a controller snapshot, after a new controller command appears\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.coproc_max_batch_size:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.coproc_max_inflight_bytes:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.coproc_max_ingest_bytes:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.coproc_offset_flush_interval_ms:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cpu_profiler_enabled:0\t- Enables cpu profiling for Redpanda\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.cpu_profiler_sample_period_ms:100\t- The sample period for the CPU profiler\r\nINFO  2025-12-10 18:54:13,"]
[0.000, "o", "887 [shard 0:main] main - application.cc:772 - redpanda.create_topic_timeout_ms:2000\t- Timeout (ms) to wait for new topic creation\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_binary_max_size:10485760\t- The maximum size for a deployable WebAssembly binary that the broker can store.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_commit_interval_ms:3000\t- The interval at which Data Transforms commits progress.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_enabled:0\t- Enables WebAssembly powered Data Transforms directly in the broker\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_logging_buffer_capacity_bytes:512000\t- Buffer capacity for transform logs, per shard. Buffer occupancy is calculated as the total size of buffered (i.e. emitted but not yet produced) log messages.\r\nINFO  2025-12-10 18:54:13,887 ["]
[0.000, "o", "shard 0:main] main - application.cc:772 - redpanda.data_transforms_logging_flush_interval_ms:500\t- Flush interval for transform logs. When a timer expires, pending logs are collected and published to the transform_logs topic.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_logging_line_max_bytes:1024\t- Transform log lines will be truncate to this length. Truncation occurs after any character escaping.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_per_core_memory_reservation:20971520\t- The amount of memory to reserve per core for Data Transform WebAssembly Virtual Machines. Memory is reserved on boot. The maximum number of functions that can be deployed to a cluster is equal to data_transforms_per_core_memory_reservation / data_transforms_per_function_memory_limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_per_function_memory_limit:2097152\t- The amount "]
[0.000, "o", "of memory to give an instance of a Data Transform WebAssembly Virtual Machine. The maximum number of functions that can be deployed to a cluster is equal to data_transforms_per_core_memory_reservation / data_transforms_per_function_memory_limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.data_transforms_runtime_limit_ms:3000\t- The maximum amount of runtime for startup time of a data transform, and the time it takes for a single record to be transformed.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.debug_load_slice_warning_depth:{nullopt}\t- The recursion depth after which debug logging will be enabled automatically for the log reader.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.default_num_windows:10\t- Default number of quota tracking windows\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.default_topic_partitions:1\t- Default number of partitions per topic\r\nINFO"]
[0.000, "o", "  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.default_topic_replications:1\t- Default replication factor for new topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.default_window_sec:1000\t- Default quota tracking window size in milliseconds\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.disable_batch_cache:0\t- Disable batch cache in log manager\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.disable_cluster_recovery_loop_for_tests:0\t- Disables the cluster recovery loop. The property exists to simplify testing and shouldn't be set in production.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.disable_metrics:0\t- Disable registering metrics exposed on the internal metrics endpoint (/metrics)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.disable_public_metrics:0\t- Disable registering metrics exposed on "]
[0.000, "o", "the public metrics endpoint (/public_metrics)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.disk_reservation_percent:25\t- The percentage of total disk capacity that Redpanda will avoid using. This applies both when cloud cache and log data share a disk, as well as when cloud cache uses a dedicated disk. It is recommended to not run disks near capacity to avoid blocking I/O due to low disk space, as well as avoiding performance issues associated with SSD garbage collection.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.election_timeout_ms:1500\t- Election timeout expressed in milliseconds\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_admin_api:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_auto_rebalance_on_node_add:0\t- Enable automatic partition rebalancing when new nodes are added\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc"]
[0.000, "o", ":772 - redpanda.enable_cluster_metadata_upload_loop:1\t- Enables the cluster metadata upload loop.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_controller_log_rate_limiting:0\t- Enables limiting of controller log write rate\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_coproc:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_idempotence:1\t- Enable idempotent producer\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_leader_balancer:1\t- Enable automatic leadership rebalancing\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_metrics_reporter:1\t- Enable cluster metrics reporter\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_mpx_extensions:0\t- Enable Redpanda extensions for MPX.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_pid_file:1\t- Enable pid file. You probably don't want to change this.\r\nINFO  2025-12-10 18:54:"]
[0.000, "o", "13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_rack_awareness:0\t- Enables rack-aware replica assignment\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_sasl:0\t- Enable SASL authentication for Kafka connections, authorization is required. see also `kafka_enable_authorization`\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_schema_id_validation:none\t- Enable Server Side Schema ID Validation.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_transactions:1\t- Enable transactions\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.enable_usage:0\t- Enables the usage tracking mechanism, storing windowed history of kafka/cloud_storage metrics over time\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.features_auto_enable:1\t- Whether new feature flags may auto-activate after upgrades (true) or must wait for manual"]
[0.000, "o", " activation via the admin API (false)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.fetch_max_bytes:57671680\t- Maximum number of bytes returned in fetch request\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.fetch_read_strategy:non_polling\t- The strategy used to fulfill fetch requests\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.fetch_reads_debounce_timeout:1\t- Time to wait for next read in fetch request when requested min bytes wasn't reached\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.fetch_session_eviction_timeout_ms:60000\t- Minimum time before which unused session will get evicted from sessions. Maximum time after which inactive session will be deleted is two time given configuration valuecache\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.find_coordinator_timeout_ms:2000\t- Time to wait for a response from tx_registry\r\nIN"]
[0.000, "o", "FO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.full_raft_configuration_recovery_pattern:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.group_initial_rebalance_delay:3000\t- Extra delay (ms) added to rebalance phase to wait for new members\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.group_max_session_timeout_ms:300000\t- The maximum allowed session timeout for registered consumers. Longer timeouts give consumers more time to process messages in between heartbeats at the cost of a longer time to detect failures. \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.group_min_session_timeout_ms:6000\t- The minimum allowed session timeout for registered consumers. Shorter timeouts result in quicker failure detection at the cost of more frequent consumer heartbeating, which can overwhelm broker resources.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 -"]
[0.000, "o", " redpanda.group_new_member_join_timeout:30000\t- Timeout for new member joins\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.group_offset_retention_check_ms:600000\t- How often the system should check for expired group offsets.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.group_offset_retention_sec:{604800000}\t- Consumer group offset retention seconds. Offset retention can be disabled by setting this value to null.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.group_topic_partitions:16\t- Number of partitions in the internal group membership topic\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.health_manager_tick_interval:180000\t- How often the health manager runs\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.health_monitor_max_metadata_age:10000\t- Max age of metadata cached in the health monitor of non controller node\r\nINFO  2025-"]
[0.000, "o", "12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.health_monitor_tick_interval:10000\t- How often health monitor refresh cluster state\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.http_authentication:{BASIC}\t- A list of supported HTTP authentication mechanisms. `BASIC` and `OIDC` are allowed.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.id_allocator_batch_size:1000\t- Id allocator allocates messages in batches (each batch is a one log record) and then serves requests from memory without touching the log until the batch is exhausted.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.id_allocator_log_capacity:100\t- Capacity of the id_allocator log in number of batches. Once it reached id_allocator_stm truncates log's prefix.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.id_allocator_replication:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main "]
[0.000, "o", "- application.cc:772 - redpanda.initial_retention_local_target_bytes_default:{nullopt}\t- Initial local retention size target for partitions of topics with cloud storage write enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.initial_retention_local_target_ms_default:{nullopt}\t- Initial local retention time target for partitions of topics with cloud storage write enabled. If no initial local target retention is configured all locally retained data will be delivered to learner when joining partition replica set\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.internal_topic_replication_factor:3\t- Target replication factor for internal topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.join_retry_timeout_ms:5000\t- Time between cluster join retries in millisecon"]
[0.000, "o", "ds\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_admin_topic_api_rate:{nullopt}\t- Target quota rate (partition mutations per default_window_sec)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_batch_max_bytes:1048576\t- Maximum size of a batch processed by server. If batch is compressed the limit applies to compressed batch size\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_client_group_byte_rate_quota:{}\t- Per-group target produce quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_client_group_fetch_byte_rate_quota:{}\t- Per-group target fetch quota byte rate (bytes per second). Client is considered part of the group if client_id contains clients_prefix\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kaf"]
[0.000, "o", "ka_connection_rate_limit:{nullopt}\t- Maximum connections per second for one core\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_connection_rate_limit_overrides:{}\t- Overrides for specific ips for maximum connections per second for one core\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_connections_max:{nullopt}\t- Maximum number of Kafka client connections per broker\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_connections_max_overrides:{}\t- Per-IP overrides of kafka connection count limit, list of <ip>:<count> strings\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_connections_max_per_ip:{nullopt}\t- Maximum number of Kafka client connections from each IP address, per broker\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_enable_authorization:{nullopt}\t- Enable authorization for Kafka connections. Values"]
[0.001, "o", ":- `nil`: Ignored. Authorization is enabled with `enable_sasl: true`; `true`: authorization is required; `false`: authorization is disabled. See also: `enable_sasl` and `kafka_api[].authentication_method`\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_enable_describe_log_dirs_remote_storage:1\t- Whether to include tiered storage as a special remote:// directory in DescribeLogDirs Kafka API requests.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_enable_partition_reassignment:1\t- Enable the Kafka partition reassignment API\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_group_recovery_timeout_ms:30000\t- Kafka group recovery timeout expressed in milliseconds\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_max_bytes_per_fetch:67108864\t- Limit fetch responses to this many bytes, even if total of partition bytes limits is higher\r\nINFO  2025-12-10 18:54"]
[0.000, "o", ":13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_memory_batch_size_estimate_for_fetch:1048576\t- The size of the batch used to estimate memory consumption for Fetch requests, in bytes. Smaller sizes allow more concurrent fetch requests per shard, larger sizes prevent running out of memory because of too many concurrent fetch requests.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_memory_share_for_fetch:0.5\t- The share of kafka subsystem memory that can be used for fetch read buffers, as a fraction of kafka subsystem memory amount\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_mtls_principal_mapping_rules:{nullopt}\t- Principal Mapping Rules for mTLS Authentication on the Kafka API\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_nodelete_topics:{_redpanda.audit_log, __consumer_offsets, _schemas}\t- Prevents the topics in the list from being deleted via the kafka api\r\nINFO "]
[0.000, "o", " 2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_noproduce_topics:{}\t- Prevents the topics in the list from having message produced to them via the kafka api\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_depth_alpha:0.8\t- Smoothing factor for kafka queue depth control depth tracking.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_depth_update_ms:7000\t- Update frequency for kafka queue depth control.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_enable:0\t- Enable kafka queue depth control.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_idle_depth:10\t- Queue depth when idleness is detected in kafka queue depth control.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_latency_alpha:0.002\t- Smoothing parameter for kafka queue depth control latency tracking.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_max_depth:100\t- Maximum queue depth used in kafka queue depth control.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_max_latency_ms:80\t- Max latency threshold for kafka queue depth control depth tracking.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_min_depth:1\t- Minimum queue depth used in kafka queue depth control.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_window_count:12\t- Number of windows used in kafka queue depth control latency tracking.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_qdc_window_size_ms:1500\t- Window size for kafka queue depth control latency tracking.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_quota_balancer_min_shard_throughput_bps:256\t- The lowest value of the throughput quota a shard can get in the process of quota balancing, in bytes/s. 0 means there is no minimum.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_quota_balancer_min_shard_throughput_ratio:0.01\t- The lowest value of the throughput quota a shard can get in the process of quota balancing, expressed as a ratio of default shard quota. 0 means there is no minimum, 1 means no quota can be taken away by the balancer.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main]"]
[0.000, "o", " main - application.cc:772 - redpanda.kafka_quota_balancer_node_period_ms:0\t- Intra-node throughput quota balancer invocation period, in milliseconds. Value of 0 disables the balancer and makes all the throughput quotas immutable.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_quota_balancer_window_ms:5000\t- Time window used to average current throughput measurement for quota balancer, in milliseconds\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_request_max_bytes:104857600\t- Maximum size of a single request processed via Kafka API\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_rpc_server_stream_recv_buf:{nullopt}\t- Userspace receive buffer max size in bytes\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_rpc_server_tcp_recv_buf:{nullopt}\t- Kafka server TCP receive buffer size in bytes.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - appli"]
[0.000, "o", "cation.cc:772 - redpanda.kafka_rpc_server_tcp_send_buf:{nullopt}\t- Kafka server TCP transmit buffer size in bytes.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_sasl_max_reauth_ms:{nullopt}\t- The maximum time between Kafka client reauthentications. If a client has not reauthenticated a connection within this time frame, that connection is torn down. Without this, a connection could live long after the client's credentials are expired or revoked. Session expiry is disabled if the value is null.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_schema_id_validation_cache_capacity:128\t- Per-shard capacity of the cache for validating schema IDs.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_tcp_keepalive_probe_interval_seconds:60000\t- TCP keepalive probe interval in seconds for kafka connections. This describes the timeout between unacknowledged tcp keepalives. Refers to the TCP_KEEPINTVL s"]
[0.000, "o", "ocket option. When changed applies to new connections only.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_tcp_keepalive_probes:3\t- TCP keepalive unacknowledged probes until the connection is considered dead for kafka connections. Refers to the TCP_KEEPCNT socket option. When changed applies to new connections only.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_tcp_keepalive_timeout:120000\t- TCP keepalive idle timeout in seconds for kafka connections. This describes the timeout between tcp keepalive probes that the remote sitesuccessfully acknowledged. Refers to the TCP_KEEPIDLE socket option. When changed applies to new connections only.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_throughput_control:{}\t- List of throughput control groups that define exclusions from node-wide throughput limits. Each group consists of: (\"name\" (optional) - any unique group name, \"client_id\" - regex "]
[0.000, "o", "to match client_id). A connection is assigned the first matching group, then the connection is excluded from throughput control.\r\n"]
[0.002, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_throughput_controlled_api_keys:{produce, fetch}\t- List of Kafka API keys that are subject to cluster-wide and node-wide throughput limit control\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_throughput_limit_node_in_bps:{nullopt}\t- Node wide throughput ingress limit - maximum kafka traffic throughput allowed on the ingress side of each node, in bytes/s. Default is no limit.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_throughput_limit_node_out_bps:{nullopt}\t- Node wide throughput egress limit - maximum kafka traffic throughput allowed on the egress side of each node, in bytes/s. Default is no limit.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_throughput_replenish_threshold:{nullopt}\t- Threshold for refilling the token bucket. Will be clamped between 1 and kafka_throughput_limit_node_*_bps.\r\nIN"]
[0.000, "o", "FO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kafka_throughput_throttling_v2:1\t- Use throughput throttling based on a shared token bucket instead of balancing quota between shards\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kvstore_flush_interval:10\t- Key-value store flush interval (ms)\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.kvstore_max_segment_size:16777216\t- Key-value maximum segment size (bytes)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.leader_balancer_idle_timeout:120000\t- Leadership rebalancing idle timeout\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.leader_balancer_mode:random_hill_climbing\t- Leader balancer mode\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.leader_balancer_mute_timeout:300000\t- Leadership rebalancing mute timeout\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.leader_balancer_transfer_limit_per_shard:512\t- Per shard limit for in progress leadership transfers\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.legacy_group_offset_retention_enabled:0\t- Group offset retention is enabled by default in versions of Redpanda >= 23.1. To enable offset retention after upgrading from an older version set this option to true.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.legacy_permit_unsafe_log_operation:1\t- Permits the use of strings that may induct log injection/modification\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.legacy_unsafe_log_warning_interval_sec:300000\t- Interval, in seconds, of how often a message informing the operator that unsafe strings are permitted\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_cleanup_policy:delete\t- Default topic"]
[0.000, "o", " cleanup policy\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_compaction_interval_ms:10000\t- How often do we trigger background compaction\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_compaction_use_sliding_window:1\t- Use sliding window compaction.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_compression_type:producer\t- Default topic compression type\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_disable_housekeeping_for_tests:0\t- Disables the housekeeping loop for local storage. The property exists to simplify testing and shouldn't be set in production.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_message_timestamp_alert_after_ms:7200000\t- Threshold in milliseconds for alerting on messages with a timestamp after the broker's time, meaning they are in the future relative to the broker's clock.\r\nINFO  2025-"]
[0.000, "o", "12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_message_timestamp_alert_before_ms:{nullopt}\t- Threshold in milliseconds for alerting on messages with a timestamp before the broker's time, meaning they are in the past relative to the broker's clock. null to disable this check\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_message_timestamp_type:CreateTime\t- Default topic messages timestamp type\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_retention_ms:604800000\t- delete segments older than this - default 1 week\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_ms:{1209600000}\t- Default log segment lifetime in ms for topics which do not set segment.ms\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_ms_max:31536000000\t- Upper bound on topic segment.ms: higher values will be clamped to this value\r\nINFO  2025-12-10 1"]
[0.000, "o", "8:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_ms_min:600000\t- Lower bound on topic segment.ms: lower values will be clamped to this value\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_size:134217728\t- Default log segment size in bytes for topics which do not set segment.bytes\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_size_jitter_percent:5\t- Random variation to the segment size limit used for each partition\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_size_max:{nullopt}\t- Upper bound on topic segment.bytes: higher values will be clamped to this limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.log_segment_size_min:{1048576}\t- Lower bound on topic segment.bytes: lower values will be clamped to this limit\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.lz4_deco"]
[0.000, "o", "mpress_reusable_buffers_disabled:0\t- Disable reusable preallocated buffers for LZ4 decompression\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.max_compacted_log_segment_size:5368709120\t- Max compacted segment size after consolidation\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.max_concurrent_producer_ids:18446744073709551615\t- Max number of the active sessions (producers). When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces - it leads to its batches being rejected with out of order sequence error.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.max_in_flight_pandaproxy_requests_per_shard:500\t- Maximum number of in flight HTTP requests permitted in pandaproxy per shard.  Any additional requests above this limit will be rejected with a 429 error\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application"]
[0.000, "o", ".cc:772 - redpanda.max_in_flight_schema_registry_requests_per_shard:500\t- Maximum number of in flight HTTP requests permitted in schema registry per shard.  Any additional requests above this limit will be rejected with a 429 error\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.max_kafka_throttle_delay_ms:30000\t- Fail-safe maximum throttle delay on kafka requests\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.max_transactions_per_coordinator:18446744073709551615\t- Max number of the active txn sessions (producers). When the threshold is passed Redpanda terminates old sessions. When an idle producer corresponding to the terminated session wakes up and produces - it leads to its batches being rejected with invalid producer epoch or invalid_producer_id_mapping (it depends on the txn execution phase).\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.max_version:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main]"]
[0.001, "o", " main - application.cc:772 - redpanda.members_backend_retry_ms:5000\t- Time between members backend reconciliation loop retries \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.memory_abort_on_alloc_failure:1\t- If true, the redpanda process will terminate immediately when an allocation cannot be satisfied due to memory exhaustion. If false, an exception is thrown instead.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.memory_enable_memory_sampling:1\t- If true, memory allocations will be sampled and tracked. A sampled live set of allocations can then be retrieved from the Admin API. Additionally, we will periodically log the top-n allocation sites\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metadata_dissemination_interval_ms:3000\t- Interval for metadata dissemination batching\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metadata_dissemination_retries:30\t- Number of "]
[0.000, "o", "attempts of looking up a topic's meta data like shard before failing a request\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metadata_dissemination_retry_delay_ms:320\t- Delay before retry a topic lookup in a shard or other meta tables\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metadata_status_wait_timeout_ms:2000\t- Maximum time to wait in metadata request for cluster health to be refreshed\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metrics_reporter_report_interval:86400000\t- cluster metrics reporter report interval\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metrics_reporter_tick_interval:60000\t- Cluster metrics reporter tick interval\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.metrics_reporter_url:https://m.rp.vectorized.io/v2\t- cluster metrics reporter url\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - applica"]
[0.000, "o", "tion.cc:772 - redpanda.min_version:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.minimum_topic_replications:1\t- Minimum permitted value of replication factor for new topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.node_isolation_heartbeat_timeout:3000\t- How long after the last heartbeat request a node will wait before considering itself to be isolated\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.node_management_operation_timeout_ms:5000\t- Timeout for executing node management operations\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.node_status_interval:100\t- Time interval between two node status messages. Node status messages establish liveness status outside of the Raft protocol.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.node_status_reconnect_max_backoff_ms:15000\t- Maximum backoff (in ms) to reconnect to an unresponsive peer during node status liveness checks.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.oidc_clock_skew_tolerance:0\t- The amount of seconds to allow for when validating the exp, nbf, and iat claims in the token.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.oidc_discovery_url:https://auth.prd.cloud.redpanda.com/.well-known/openid-configuration\t- The URL pointing to the well-known discovery endpoint for the OIDC provider.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.oidc_keys_refresh_interval:3600000\t- The frequency of refreshing the JSON Web Keys (JWKS) used to validate access tokens.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.oidc_principal_mapping:$.sub\t- Rule for mapping JWT Payload claim to a Redpanda User Principal\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.oidc_token_audience:redpanda\t- A string representing the intended recipient of the token.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_concurrent_moves:50\t- Number of partitions that can be reassigned at once\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_max_disk_usage_percent:80\t- Disk usage threshold that triggers moving partitions from the node\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_min_size_threshold:{nullopt}\t- Minimum size of partition that is going to be prioritized when rebalancing cluster due to disk size threshold being breached. By default this value is calculated automaticaly\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_mode:node_add\t- Partition autobalancing mode\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_movement_batch_size_bytes:5368709120\t- Total size of partitions that a"]
[0.000, "o", "utobalancer is going to move in one batch (deprecated, use partition_autobalancing_concurrent_moves to limit the autobalancer concurrency)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_node_availability_timeout_sec:900000\t- Node unavailability timeout that triggers moving partitions from the node\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_tick_interval_ms:30000\t- Partition autobalancer tick interval\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_tick_moves_drop_threshold:0.2\t- If the number of scheduled tick moves drops by this ratio, a new tick is scheduled immediately. Valid values are (0, 1]. For example, with a value of 0.2 and 100 scheduled moves in a tick, a new tick is scheduled when the inprogress moves are < 80.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_autobalancing_t"]
[0.000, "o", "opic_aware:1\t- If true, Redpanda will prioritize balancing topic-wise number of partitions on each node, as opposed to balancing the total number of partitions. This should give better balancing results if topics with diverse partition sizes and load profiles are present in the cluster.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.partition_manager_shutdown_watchdog_timeout:30000\t- A threshold value to detect partitions which shutdown might have been stuck. After this threshold a watchdog in partition manager will log information about partition shutdown not making progress\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.pp_sr_smp_max_non_local_requests:{nullopt}\t- Maximum number of x-core requests pending in Panda Proxy and Schema Registry seastar::smp group.  (for more details look at `seastar::smp_service_group` documentation)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.quota_manager_gc_sec:30000"]
[0.000, "o", "\t- Quota manager GC frequency in milliseconds\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_enable_longest_log_detection:1\t- Enables additional step in leader election where candidate is allowed to wait for all the replies from node it requested votes from. This may introduce a small delay when recovering from failure but will prevent truncation if any of the replicas has more data than the majority.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_enable_lw_heartbeat:1\t- enables raft optimization of heartbeats\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_flush_timer_interval_ms:100\t- Interval of checking partition against the `raft_replica_max_pending_flush_bytes`, deprecated started 24.1, use raft_replica_max_flush_delay_ms instead \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_heartbeat_disconnect_failures:3\t- After how many failed heartbeats "]
[0.000, "o", "to forcibly close an unresponsive TCP connection.  Set to 0 to disable force disconnection.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_heartbeat_interval_ms:150\t- Milliseconds for raft leader heartbeats\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_heartbeat_timeout_ms:3000\t- raft heartbeat RPC timeout\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_io_timeout_ms:10000\t- Raft I/O timeout\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_learner_recovery_rate:104857600\t- Raft learner recovery rate limit in bytes per sec\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_max_concurrent_append_requests_per_follower:16\t- Maximum number of concurrent append entries requests sent by leader to one follower\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_max_recovery_memory:{null"]
[0.000, "o", "opt}\t- Max memory that can be used for reads in raft recovery process by default 15% of total memory\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_recovery_concurrency_per_shard:64\t- How many partitions may simultaneously recover data to a particular shard. This is limited to avoid overwhelming nodes when they come back online after an outage.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_recovery_default_read_size:524288\t- default size of read issued during raft follower recovery\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_recovery_throttle_disable_dynamic_mode:0\t- Disables dynamic rate allocation in recovery throttle (advanced).\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_replica_max_flush_delay_ms:100\t- Maximum delay (in ms) between two subsequent flushes. After this delay, the log will be automatically force flushed.\r\nINFO  2025-12-10 1"]
[0.000, "o", "8:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_replica_max_pending_flush_bytes:{262144}\t- Max not flushed bytes per partition. If configured threshold is reached log will automatically be flushed even though it wasn't explicitly requested\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_replicate_batch_window_size:1048576\t- Max size of requests cached for replication\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_smp_max_non_local_requests:{nullopt}\t- Maximum number of x-core requests pending in Raft seastar::smp group. (for more details look at `seastar::smp_service_group` documentation)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_timeout_now_timeout_ms:1000\t- Timeout for a timeout now request\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.raft_transfer_leader_recovery_timeout_ms:10000\t- Timeout waiting for follower recovery when"]
[0.000, "o", " transferring leadership\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.readers_cache_eviction_timeout_ms:30000\t- Duration after which inactive readers will be evicted from cache\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.readers_cache_target_max_size:200\t- Maximum desired number of readers cached per ntp. This a soft limit, a number of readers in cache may temporary increase as cleanup is done in background\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.reclaim_batch_cache_min_free:67108864\t- Free memory limit that will be kept by batch cache background reclaimer\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.reclaim_growth_window:3000\t- Length of time in which reclaim sizes grow\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.reclaim_max_size:4194304\t- Maximum batch cache reclaim size\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:7"]
[0.000, "o", "72 - redpanda.reclaim_min_size:131072\t- Minimum batch cache reclaim size\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.reclaim_stable_window:10000\t- Length of time above which growth is reset\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.recovery_append_timeout_ms:5000\t- Timeout for append entries requests issued while updating stale follower\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.release_cache_on_segment_roll:0\t- Free cache when segments roll\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.replicate_append_timeout_ms:3000\t- Timeout for append entries requests issued while replicating entries\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_bytes:{nullopt}\t- Default max bytes per partition on disk before triggering a compaction\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retent"]
[0.001, "o", "ion_local_strict:0\t- Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_strict_override:1\t- Trim log data when a cloud topic reaches its local retention limit. When this option is disabled Redpanda will allow partitions to grow past the local retention limit, and will be trimmed automatically as storage reaches the configured target size.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_target_bytes_default:{nullopt}\t- Local retention size target for partitions of topics with cloud storage write enabled\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_target_capacity_bytes:{nullopt}\t- The target capacity in bytes that l"]
[0.000, "o", "og storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_target_capacity_percent:{80}\t- The target capacity in percent of unreserved space (see disk_reservation_percent) that log storage will try to use before additional retention rules will take over to trim data in order to meet the target. When no target is specified storage usage is unbounded.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_target_ms_default:86400000\t- Local retention time target for partitions of topics with cloud storage write enabled\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_trim_interval:30000\t- The maximum amount of time before log storage will examine usage to determine of the target capacity has been exceeded and "]
[0.000, "o", "additional data trimming is required.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.retention_local_trim_overage_coeff:2\t- The space management control loop will reclaim the overage multiplied by this this coefficient in order to compensate for data that is written during the idle period between control loop invocations.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rm_sync_timeout_ms:10000\t- Time to wait state catch up before rejecting a request\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rm_violation_recovery_policy:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rpc_client_connections_per_peer:32\t- The maximum number of connections a broker will open to each of its peers\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rpc_server_compress_replies:0\t- Enable compression for internal rpc server replies\r\nINFO  2025-12-10 18:54"]
[0.000, "o", ":13,887 [shard 0:main] main - application.cc:772 - redpanda.rpc_server_listen_backlog:{nullopt}\t- TCP connection queue length for Kafka server and internal RPC server\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rpc_server_tcp_recv_buf:{nullopt}\t- Internal RPC TCP receive buffer size in bytes.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rpc_server_tcp_send_buf:{nullopt}\t- Internal RPC TCP transmit buffer size in bytes.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rps_limit_acls_and_users_operations:1000\t- Rate limit for controller acls and users operations\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rps_limit_configuration_operations:1000\t- Rate limit for controller configuration operations\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rps_limit_move_operations:1000\t- Rate limit for controller move operations\r\nINFO  2025-"]
[0.000, "o", "12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rps_limit_node_management_operations:1000\t- Rate limit for controller node management operations\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.rps_limit_topic_operations:1000\t- Rate limit for controller topic operations\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.sasl_kerberos_config:/etc/krb5.conf\t- The location of the Kerberos krb5.conf file for Redpanda\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.sasl_kerberos_keytab:/var/lib/redpanda/redpanda.keytab\t- The location of the Kerberos keytab file for Redpanda\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.sasl_kerberos_principal:redpanda\t- The primary of the Kerberos Service Principal Name (SPN) for Redpanda\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.sasl_kerberos_principal_mapping:{DEFAULT}\t- Rules for"]
[0.000, "o", " mapping Kerberos Principal Names to Redpanda User Principals\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.sasl_mechanisms:{SCRAM}\t- A list of supported SASL mechanisms. `SCRAM`, `GSSAPI`, and `OAUTHBEARER` are allowed.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.seed_server_meta_topic_partitions:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.segment_appender_flush_timeout_ms:1000\t- Maximum delay until buffered data is written\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.segment_fallocation_step:33554432\t- Size for segments fallocation\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.seq_table_min_size:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.space_management_enable:1\t- Enable automatic space management.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - r"]
[0.000, "o", "edpanda.space_management_enable_override:0\t- Enable automatic space management. This option is ignored and deprecated in versions >= v23.3.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.space_management_max_log_concurrency:20\t- Maximum parallel logs inspected during space management process.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.space_management_max_segment_concurrency:10\t- Maximum parallel segments inspected during space management process.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_compaction_index_memory:134217728\t- Maximum number of bytes that may be used on each shard by compactionindex writers\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_compaction_key_map_memory:134217728\t- Maximum number of bytes that may be used on each shard by compaction key-offset maps. Only respected when `log_compaction_use_sliding_window` is true.\r\nINFO  "]
[0.000, "o", "2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_compaction_key_map_memory_limit_percent:12\t- Limit on `storage_compaction_key_map_memory`, expressed as a percentage of memory per shard, that bounds the amount of memory used by compaction key-offset maps. NOTE: Memory per shard is computed after `wasm_per_core_memory_reservation`. Only respected when `log_compaction_use_sliding_window` is true.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_ignore_cstore_hints:0\t- if set, cstore hints will be ignored and will not be used for data access (but will otherwise be generated)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_ignore_timestamps_in_future_sec:{nullopt}\t- If set, timestamps more than this many seconds in the future relative tothe server's clock will be ignored for data retention purposes, and retention will act based on another timestamp in the same segment, or the mtime of the segmen"]
[0.000, "o", "t file if no valid timestamp is available\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_max_concurrent_replay:1024\t- Maximum number of partitions' logs that will be replayed concurrently at startup, or flushed concurrently on shutdown.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_min_free_bytes:5368709120\t- Threshold of minimum bytes free space before rejecting producers.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_read_buffer_size:131072\t- Size of each read buffer (one per in-flight read, per log segment)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_read_readahead_count:10\t- How many additional reads to issue ahead of current read location\r\n"]
[0.001, "o", "INFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_reserve_min_segments:2\t- The number of segments per partition that the system will attempt to reserve disk capacity for. For example, if the maximum segment size is configured to be 100 MB, and the value of this option is 2, then in a system with 10 partitions Redpanda will attempt to reserve at least 2 GB of disk space.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_space_alert_free_threshold_bytes:0\t- Threshold of minimum bytes free space before setting storage space alert\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_space_alert_free_threshold_percent:5\t- Threshold of minimum percent free space before setting storage space alert\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_strict_data_init:0\t- Requires that an empty file named `.redpanda_data_dir` be present in the data directory. Redp"]
[0.000, "o", "anda will refuse to start if it is not found.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.storage_target_replay_bytes:10737418240\t- Target bytes to replay from disk on startup after clean shutdown: controls frequency of snapshots and checkpoints\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.superusers:{}\t- List of superuser usernames\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.target_fetch_quota_byte_rate:{nullopt}\t- Target fetch size quota byte rate (bytes per second) - disabled default\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.target_quota_byte_rate:2147483648\t- Target request size quota byte rate (bytes per second) - 2GB default\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.tm_sync_timeout_ms:10000\t- Time to wait state catch up before rejecting a request\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - applicatio"]
[0.000, "o", "n.cc:772 - redpanda.tm_violation_recovery_policy:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.topic_fds_per_partition:{5}\t- Required file handles per partition when creating topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.topic_memory_per_partition:{4194304}\t- Required memory per partition when creating topics\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.topic_partitions_per_shard:1000\t- Maximum number of partitions which may be allocated to one shard (CPU core)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.topic_partitions_reserve_shard0:2\t- Reserved partition slots on shard (CPU core) 0 on each node.  If this is >= topic_partitions_per_core, no data partitions will be scheduled on shard 0\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.transaction_coordinator_cleanup_policy:delete\t- Cleanup policy for a transaction c"]
[0.000, "o", "oordinator topic\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.transaction_coordinator_delete_retention_ms:604800000\t- delete segments older than this - default 1 week\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.transaction_coordinator_log_segment_size:1073741824\t- How large in bytes should each log segment be (default 1G)\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.transaction_coordinator_partitions:50\t- Amount of partitions for transactions coordinator\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.transaction_coordinator_replication:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.transactional_id_expiration_ms:604800000\t- Producer ids are expired once this time has elapsed after the last write with the given producer id.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.tx_log_stats_inte"]
[0.001, "o", "rval_s:10000\t- How often to log per partition tx stats, works only with debug logging enabled.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.tx_registry_log_capacity:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.tx_registry_sync_timeout_ms:\t- \r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.tx_timeout_delay_ms:1000\t- Delay before scheduling next check for timed out transactions\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.unsafe_enable_consumer_offsets_delete_retention:0\t- Enables delete retention of consumer offsets topic. This is an internal-only configuration and should be enabled only after consulting with Redpanda Support or engineers.\r\nINFO  2025-12-10 18:54:13,887 [shard 0:main] main - application.cc:772 - redpanda.usage_disk_persistance_interval_sec:300000\t- The interval in which all usage stats are written to disk\r\nINFO  2025-12-10 18:54:13,887 [shard "]
[0.000, "o", "0:main] main - application.cc:772 - redpanda.usage_num_windows:24\t- The number of windows to persist in memory and disk\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.usage_window_width_interval_sec:3600000\t- The width of a usage window, tracking cloud and kafka ingress/egress traffic each interval\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.use_fetch_scheduler_group:1\t- Use a separate scheduler group for fetch processing\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.use_scheduling_groups:\t- \r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.virtual_cluster_min_producer_ids:18446744073709551615\t- Minimum number of active producers per virtual cluster\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.wait_for_leader_timeout_ms:5000\t- Timeout (ms) to wait for leadership in metadata cache\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.write_caching_default:false\t- Cache batches until the segment appender chunk is full instead of flushing for every acks=all write. This is the global default for all topics and can be overriden at a topic scope with property write.caching. 'disabled' mode takes precedence over topic overrides and disables the feature altogether for the entire cluster.\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.zstd_decompress_workspace_bytes:8388608\t- Size of the zstd decompression workspace\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:814 - Node configuration properties:\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:815 - (use `rpk redpanda config set <cfg> <value>` to change)\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.admin:{{:{host: 0.0.0.0, port: 9644}}}\t- Address and port of admin server\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.admin_api_doc_dir:/usr/share/redpanda/admin-api-doc\t- Admin API doc directory\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.admin_api_tls:{}\t- TLS configuration for admin HTTP server\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.advertised_kafka_api:{{:{host: 127.0.0.1, port: 9092}}}\t- Address of Kafka API published to the clients\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.advertised_rpc_api:{{host: 127.0.0.1, port: 33145}}\t- Address of RPC endpoint published to other cluster members\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.cloud_storage_cache_directory:{nullopt}\t- Directory for archival cache. Should be present when `cloud_storage_enabled` is present\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.coproc_supervisor_server:\t- \r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.crash_loop_limit:{5}\t- Maximum consecutive crashes (unclean shutdowns) allowed after which operator intervention is needed to startup the broker. Limit is not enforced in developer mode.\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.dashboard_dir:\t- \r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.data_directory:{data_directory=\"/var/lib/conduktor/redpanda\"}\t- Place where redpanda will keep the data\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.developer_mode:1\t- Skips most of the checks performed at startup, not recomended for production use\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.emergency_disable_data_transforms:0\t- Override the cluster enablement setting and disable"]
[0.000, "o", " WebAssembly powered data transforms. Only used as an emergency shutoff button.\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.empty_seed_starts_cluster:1\t- If true, an empty seed_servers list will denote that this node should form a cluster. At most one node in the cluster should be configured configured with an empty seed_servers list. If no such configured node exists, or if configured to false, all nodes denoted by the seed_servers list must be identical among those nodes' configurations, and those nodes will form the initial cluster.\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.enable_central_config:\t- \r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.kafka_api:{{:{host: 0.0.0.0, port: 9092}:{nullopt}}}\t- Address and port of an interface to listen for Kafka API requests\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.kafka_api_tls:{}\t- TLS configuration for Kafka API endpoint\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.memory_allocation_warning_threshold:{131073}\t- Enables log messages for allocations greater than the given size.\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.node_id:{nullopt}\t- Unique id identifying a node in the cluster. If missing, a unique id will be assigned for this node when it joins the cluster\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.rack:{nullopt}\t- Rack identifier\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.recovery_mode_enabled:0\t- If true, start redpanda in \""]
[0.000, "o", "metadata only\" mode, skipping loading user partitions and allowing only metadata operations.\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.rpc_server:{host: 0.0.0.0, port: 33145}\t- IpAddress and port for RPC server\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.rpc_server_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} client_auth_required: 0 }\t- TLS configuration for RPC server\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.seed_servers:{}\t- List of the seed servers used to join current cluster. If the seed_server list is empty the node will be a cluster root and it will form a new cluster\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.storage_failure_injection_config_path:{nullopt}\t- Path to the configuration file used for low level storage failure injection\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.storage_fai"]
[0.000, "o", "lure_injection_enabled:0\t- If true, inject low level storage failures on the write path. **Not** for production usage.\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.upgrade_override_checks:0\t- Whether to violate safety checks when starting a redpanda version newer than the cluster's consensus version\r\nINFO  2025-12-10 18:54:13,888 [shard 0:main] main - application.cc:772 - redpanda.verbose_logging_timeout_sec_max:{nullopt}\t- Maximum duration in seconds for verbose (i.e. TRACE or DEBUG) logging. Values configured above this will be clamped. If null (the default) there is no limit. Can be overridded in the Admin API on a per-request basis.\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy.advertised_pandaproxy_api:{}\t- Rest API address and port to publish to client\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy.api_doc_dir:/usr/share/redpanda/proxy-api-doc\t- API doc directory\r\nINFO  2025-12-10 18:54:13,"]
[0.000, "o", "889 [shard 0:main] main - application.cc:772 - pandaproxy.client_cache_max_size:10\t- The maximum number of kafka clients in the LRU cache\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy.client_keep_alive:300000\t- Time in milliseconds that an idle connection may remain open\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy.consumer_instance_timeout_ms:300000\t- How long to wait for an idle consumer before removing it\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy.pandaproxy_api:{{:{host: 0.0.0.0, port: 8082}:<nullopt>}}\t- Rest API listen address and port\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy.pandaproxy_api_tls:{}\t- TLS configuration for Pandaproxy api\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} client_auth_required: 0 }\t- TLS configuration fo"]
[0.000, "o", "r the brokers\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.brokers:{{host: 0.0.0.0, port: 9092}}\t- List of address and port of the brokers\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.client_identifier:{pandaproxy_client}\t- Identifier to use within the kafka request header\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.consumer_heartbeat_interval_ms:500\t- Interval (in milliseconds) for consumer heartbeats\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.consumer_rebalance_timeout_ms:2000\t- Timeout (in milliseconds) for consumer rebalance\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.consumer_request_max_bytes:1048576\t- Max bytes to fetch per request\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.consumer_request_min_bytes:1\t- Min bytes to fe"]
[0.000, "o", "tch per request\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.consumer_request_timeout_ms:100\t- Interval (in milliseconds) for consumer request timeout\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.consumer_session_timeout_ms:300000\t- Timeout (in milliseconds) for consumer session\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.produce_ack_level:-1\t- Number of acknowledgments the producer requires the leader to have received before considering a request complete, choices are 0, 1 and -1\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.produce_batch_delay_ms:100\t- Delay (in milliseconds) to wait before sending batch\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.produce_batch_record_count:1000\t- Number of records to batch before sending to broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main"]
[0.000, "o", "] main - application.cc:772 - pandaproxy_client.produce_batch_size_bytes:1048576\t- Number of bytes to batch before sending to broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.produce_compression_type:none\t- Enable or disable compression by the kafka client. Specify 'none' to disable compression or one of the supported types [gzip, snappy, lz4, zstd]\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.produce_shutdown_delay_ms:0\t- Delay (in milliseconds) to allow for final flush of buffers before shutting down\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.retries:5\t- Number of times to retry a request to a broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.retry_base_backoff_ms:100\t- Delay (in milliseconds) for initial retry backoff\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.sasl_m"]
[0.000, "o", "echanism:\t- The SASL mechanism to use when connecting\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.scram_password:\t- Password to use for SCRAM authentication mechanisms\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - pandaproxy_client.scram_username:\t- Username to use for SCRAM authentication mechanisms\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry.api_doc_dir:/usr/share/redpanda/proxy-api-doc\t- API doc directory\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry.mode_mutability:1\t- Allow modifying mode\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry.schema_registry_api:{{:{host: 0.0.0.0, port: 8081}:<nullopt>}}\t- Schema Registry API listen address and port\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry.schema_registry_api_tls:{}\t- TLS configuration for Schema Registry API"]
[0.001, "o", "\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry.schema_registry_replication_factor:{nullopt}\t- Replication factor for internal _schemas topic.  If unset, defaults to `default_topic_replication`\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} client_auth_required: 0 }\t- TLS configuration for the brokers\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.brokers:{{host: 0.0.0.0, port: 9092}}\t- List of address and port of the brokers\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.client_identifier:{schema_registry_client}\t- Identifier to use within the kafka request header\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.consumer_heartbeat_interval_ms:500\t- Interval (in milliseconds) for consumer heartbeats\r\nINFO"]
[0.000, "o", "  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.consumer_rebalance_timeout_ms:2000\t- Timeout (in milliseconds) for consumer rebalance\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.consumer_request_max_bytes:1048576\t- Max bytes to fetch per request\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.consumer_request_min_bytes:1\t- Min bytes to fetch per request\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.consumer_request_timeout_ms:100\t- Interval (in milliseconds) for consumer request timeout\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.consumer_session_timeout_ms:10000\t- Timeout (in milliseconds) for consumer session\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.produce_ack_level:-1\t- Number of acknowledgments the producer requires the leader to have received before considering a request complete, choices are 0, 1 and -1\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.produce_batch_delay_ms:0\t- Delay (in milliseconds) to wait before sending batch\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.produce_batch_record_count:0\t- Number of records to batch before sending to broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.produce_batch_size_bytes:0\t- Number of bytes to batch before sending to broker\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.produce_compression_type:none\t- Enable or disable compression by the kafka client. Specify 'none' to disable compression or one of the supported types [gzip, snappy, lz4, zstd]\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.produce_shutdown_delay_ms:0\t- Delay (in milliseconds) to allow for final flush of buffers before shutting down\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.retries:5\t- Number of times to retry a request to a broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.retry_base_backoff_ms:100\t- Delay (in milliseconds) for initial retry backoff\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.sasl_mechanism:\t- The SASL mechanism to use when connecting\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - ap"]
[0.000, "o", "plication.cc:772 - schema_registry_client.scram_password:\t- Password to use for SCRAM authentication mechanisms\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - schema_registry_client.scram_username:\t- Username to use for SCRAM authentication mechanisms\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.broker_tls:{ enabled: 0 key/cert files: {nullopt} ca file: {nullopt} client_auth_required: 0 }\t- TLS configuration for the brokers\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.brokers:{{host: 0.0.0.0, port: 9092}}\t- List of address and port of the brokers\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.client_identifier:{audit_log_client}\t- Identifier to use within the kafka request header\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.consumer_heartbeat_interval_ms:500\t- Interval (in milliseconds) for consumer hear"]
[0.000, "o", "tbeats\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.consumer_rebalance_timeout_ms:2000\t- Timeout (in milliseconds) for consumer rebalance\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.consumer_request_max_bytes:1048576\t- Max bytes to fetch per request\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.consumer_request_min_bytes:1\t- Min bytes to fetch per request\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.consumer_request_timeout_ms:100\t- Interval (in milliseconds) for consumer request timeout\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.consumer_session_timeout_ms:10000\t- Timeout (in milliseconds) for consumer session\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.produce_ack_level:1\t- Number of acknowledgments the producer requires the leader t"]
[0.000, "o", "o have received before considering a request complete, choices are 0, 1 and -1\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.produce_batch_delay_ms:0\t- Delay (in milliseconds) to wait before sending batch\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.produce_batch_record_count:0\t- Number of records to batch before sending to broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.produce_batch_size_bytes:0\t- Number of bytes to batch before sending to broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.produce_compression_type:zstd\t- Enable or disable compression by the kafka client. Specify 'none' to disable compression or one of the supported types [gzip, snappy, lz4, zstd]\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.produce_shutdown_delay_ms:3000\t- Delay (in milliseconds) to allow for f"]
[0.000, "o", "inal flush of buffers before shutting down\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.retries:5\t- Number of times to retry a request to a broker\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.retry_base_backoff_ms:100\t- Delay (in milliseconds) for initial retry backoff\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.sasl_mechanism:\t- The SASL mechanism to use when connecting\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.scram_password:\t- Password to use for SCRAM authentication mechanisms\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:772 - audit_log_client.scram_username:\t- Username to use for SCRAM authentication mechanisms\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] seastar - Enabling heap profiler - using 3000037 bytes sampling rate\r\nINFO  2025-12-10 18:54:13,889 [shard 0:main] main - application.cc:506 - "]
[0.000, "o", "Setting abort_on_allocation_failure (abort on OOM): true\r\nINFO  2025-12-10 18:54:13,892 [shard 0:main] syschecks - Writing pid file \"/var/lib/conduktor/redpanda/pid.lock\"\r\nINFO  2025-12-10 18:54:13,899 [shard 0:main] storage - directories.h:33 - Checking `/var/lib/conduktor/redpanda` for supported filesystems\r\nINFO  2025-12-10 18:54:13,899 [shard 0:main] syschecks - Detected file system type is other\r\nERROR 2025-12-10 18:54:13,899 [shard 0:main] syschecks - Path: `/var/lib/conduktor/redpanda' uses other filesystem which is not XFS or ext4. This is a unsupported configuration. You may experience poor performance or instability.\r\nINFO  2025-12-10 18:54:13,899 [shard 0:main] cloud_storage - cache_service.cc:1642 - Creating cache directory \"/var/lib/conduktor/redpanda/cloud_storage_cache\"\r\nINFO  2025-12-10 18:54:14,024 [shard 0:main] rpc - server.cc:41 - Creating net::server for internal_rpc with config {{://0.0.0.0:33145:PLAINTEXT}, max_service_memory_per_core: 188978560, metrics_enabled:true, listen_backlog:{nu"]
[0.000, "o", "llopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}\r\nINFO  2025-12-10 18:54:14,030 [shard 0:main] features - feature_table.cc:360 - Activating features from bootstrap version 7\r\nINFO  2025-12-10 18:54:14,030 [shard 0:main] main - application.cc:2335 - Generated new UUID for node: 44f658f7-136e-4c52-b045-d9a07608321a\r\nINFO  2025-12-10 18:54:14,041 [shard 0:main] storage - segment.cc:806 - Creating new segment /var/lib/conduktor/redpanda/redpanda/kvstore/0_0/0-0-v1.log\r\nINFO  2025-12-10 18:54:14,044 [shard 0:main] main - application.cc:2365 - Started RPC server listening at {host: 0.0.0.0, port: 33145}\r\nINFO  2025-12-10 18:54:14,045 [shard 0:main] main - application.cc:2459 - Starting Redpanda with node_id 0, cluster UUID {nullopt}\r\nINFO  2025-12-10 18:54:14,046 [shard 0:main] raft - coordinated_recovery_throttle.cc:124 - Starting recovery throttle, rate: 104857600\r\nINFO  2025-12-10 18:54:14,046 [shard 0:main] cluster - producer_state_manager.cc:45 - Started producer state manager"]
[0.000, "o", "\r\nINFO  2025-12-10 18:54:14,046 [shard 0:main] main - application.cc:1485 - Partition manager started\r\nINFO  2025-12-10 18:54:14,048 [shard 0:main] main - application.cc:1573 - Archiver service setup, cloud_storage_enabled: false, legacy_upload_mode_enabled: true\r\nINFO  2025-12-10 18:54:14,049 [shard 0:main] resource_mgmt - storage.cc:182 - Setting new target log data size 201.768GiB. Disk size 336.280GiB reservation percent 25 target percent {80} bytes {nullopt}\r\nINFO  2025-12-10 18:54:14,051 [shard 0:main] kafka - server.cc:41 - Creating net::server for kafka_rpc with config {{://0.0.0.0:9092:PLAINTEXT}, max_service_memory_per_core: 283467840, metrics_enabled:true, listen_backlog:{nullopt}, tcp_recv_buf:{nullopt}, tcp_send_buf:{nullopt}, stream_recv_buf:{nullopt}}\r\nINFO  2025-12-10 18:54:14,066 [shard 0:main] cluster - raft0_utils.h:30 - Current node is a cluster founder\r\nINFO  2025-12-10 18:54:14,099 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1418 - Starting with voted_for {id"]
[0.000, "o", ": -2147483648, revision: -9223372036854775808} term 0 initial_state true\r\nINFO  2025-12-10 18:54:14,125 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1462 - Current log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, read bootstrap state: data_seen 0 config_seen 0 eol false commit 0 term 0 prev_idx 0 prev_term 0 config_tracker -9223372036854775808 commit_base_tracker -9223372036854775808 configurations {}\r\nINFO  2025-12-10 18:54:14,125 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:1484 - Truncating configurations at -9223372036854775808\r\nINFO  2025-12-10 18:54:14,159 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] consensus.cc:927 - starting pre-vote leader election, current term: 0, leadership transfer: false\r\nINFO  2025-12-10 18:54:14,167 [shard 0:main] raft - [group_id:0, {redpanda/contro"]
[0.000, "o", "ller/0}] consensus.cc:1587 - started raft, log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, term: 0, configuration: {current: {voters: {{id: 0, revision: 0}}, learners: {}}, old:{nullopt}, revision: 0, update: {nullopt}, version: 4}, brokers: {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 1073741824, disk_available 336}}}}\r\nINFO  2025-12-10 18:54:14,174 [shard 0:main] cluster - drain_manager.cc:21 - Drain manager starting\r\nINFO  2025-12-10 18:54:14,175 [shard 0:main] cluster - members_manager.cc:99 - starting  members manager with founding brokers: {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 1073741824, dis"]
[0.000, "o", "k_available 336}}}\r\nINFO  2025-12-10 18:54:14,175 [shard 0:main] cluster - controller.cc:506 - Controller log replay starting (to offset -9223372036854775808)\r\nINFO  2025-12-10 18:54:14,175 [shard 0:main] cluster - controller.cc:518 - Controller log replay complete.\r\nINFO  2025-12-10 18:54:14,180 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:418 - becoming the leader term:1\r\nINFO  2025-12-10 18:54:14,182 [shard 0:main] storage - segment.cc:806 - Creating new segment /var/lib/conduktor/redpanda/redpanda/controller/0_0/0-1-v1.log\r\nINFO  2025-12-10 18:54:14,217 [shard 0:main] cluster - members_manager.cc:209 - processing raft-0 configuration at offset: 0 with brokers: [{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 0, disk_available 336}}]\r\n"]
[0.001, "o", "INFO  2025-12-10 18:54:14,221 [shard 0:main] cluster - members_table.cc:87 - adding node {id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 0, disk_available 336}}\r\nINFO  2025-12-10 18:54:14,222 [shard 0:main] raft - [group_id:0, {redpanda/controller/0}] vote_stm.cc:433 - became the leader term: 1\r\nINFO  2025-12-10 18:54:14,276 [shard 0:main] cluster - controller.cc:901 - Creating cluster UUID 8d497f9a-a863-4dc5-9395-3b8521141d1d\r\nINFO  2025-12-10 18:54:14,276 [shard 0:main] cluster - bootstrap_backend.cc:92 - Applying update to bootstrap_manager\r\nINFO  2025-12-10 18:54:14,277 [shard 0:main] cluster - members_manager.cc:848 - Initial node UUID map: {44f658f7-136e-4c52-b045-d9a07608321a: 0}\r\nINFO  2025-12-10 18:54:14,277 [shard 0:main] cluster - members_table.cc:98 - setting initial nodes {{id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, po"]
[0.000, "o", "rt: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 1073741824, disk_available 336}}}\r\nINFO  2025-12-10 18:54:14,288 [shard 0:main] features - feature_table.cc:360 - Activating features from bootstrap version 12\r\nINFO  2025-12-10 18:54:14,289 [shard 0:main] cluster - feature_backend.cc:149 - Saving feature_table_snapshot at version 12...\r\nINFO  2025-12-10 18:54:14,313 [shard 0:main] cluster - controller.cc:909 - Cluster UUID created 8d497f9a-a863-4dc5-9395-3b8521141d1d\r\nINFO  2025-12-10 18:54:14,314 [shard 0:main] cluster - controller_backend.cc:761 - Cleaning up orphan topic files. bootstrap_revision: -9223372036854775808\r\nINFO  2025-12-10 18:54:14,323 [shard 0:main] cluster - feature_manager.cc:90 - Starting...\r\nINFO  2025-12-10 18:54:14,325 [shard 0:main] cluster - feature_manager.cc:501 - Activating features after upgrade...\r\nINFO  2025-12-10 18:54:14,325 [shard 0:main] cluster - feature_manager.cc:510 - Activating feature broker_time_based_retention (logical version 12)\r\nINFO  2025-12-10 18:"]
[0.000, "o", "54:14,325 [shard 0:main] cluster - metrics_reporter.cc:278 - Waiting to initialize cluster metrics ID...\r\nINFO  2025-12-10 18:54:14,325 [shard 0:main] cluster - partition_balancer_backend.cc:101 - partition balancer started\r\nINFO  2025-12-10 18:54:14,325 [shard 0:main] cluster - leader_balancer.cc:104 - Leader balancer: controller leadership detected. Starting rebalancer in 30 seconds\r\nWARN  2025-12-10 18:54:14,336 [shard 0:main] admin_api_server - server.cc:531 - Insecure Admin API listener on 0.0.0.0:9644, consider enabling `admin_api_require_auth`\r\nINFO  2025-12-10 18:54:14,336 [shard 0:main] admin_api_server - server.cc:343 - Started HTTP admin service listening at {{:{host: 0.0.0.0, port: 9644}}}\r\nINFO  2025-12-10 18:54:14,336 [shard 0:main] resource_mgmt - storage.cc:73 - Starting disk space manager service (enabled)\r\nINFO  2025-12-10 18:54:14,339 [shard 0:main] main - application.cc:2497 - Started Pandaproxy listening at {{:{host: 0.0.0.0, port: 8082}:<nullopt>}}\r\nINFO  2025-12-10 18:54:14,342 [shard 0"]
[0.000, "o", ":main] main - application.cc:2505 - Started Schema Registry listening at {{:{host: 0.0.0.0, port: 8081}:<nullopt>}}\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:14,342 [shard 0:main] main - application.cc:2831 - Waiting for cluster membership\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:14,342 [shard 0:main] main - application.cc:2849 - Waiting for Cluster ID to initialize...\r\nINFO  2025-12-10 18:54:14,347 [shard 0:main] cluster - metrics_reporter.cc:322 - Generated cluster metrics ID 73146e35-013c-4558-b73a-17e0eb924737\r\nINFO  2025-12-10 18:54:14,348 [shard 0:main] cluster - feature_backend.cc:149 - Saving feature_table_snapshot at version 12...\r\nINFO  2025-12-10 18:54:14,348 [shard 0:main] cluster - config_manager.cc:136 - Completed bootstrap as leader\r\nINFO  2025-12-10 18:54:14,348 [shard 0:main] cluster - config_manager.cc:124 - Bootstrap complete (version 1)\r\nINFO  2025-12-10 18:54:14,360 [shard 0:main] cluster - members_manager.cc:442 - processing node update command - broker: {id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 1073741824, disk_available 336}}, offset: 4\r\nINFO  2025-12-10 18:54:14,360 [shard 0:main] cluster - members_manager.cc:54"]
[0.000, "o", "3 - processing node update command - broker: {id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 1073741824, disk_available 336}}, offset: 4\r\nINFO  2025-12-10 18:54:14,360 [shard 0:main] cluster - members_table.cc:113 - updating node configuration {id: 0, kafka_advertised_listeners: {{:{host: 127.0.0.1, port: 9092}}}, rpc_address: {host: 127.0.0.1, port: 33145}, rack: {nullopt}, properties: {cores 1, mem_available 1073741824, disk_available 336}}\r\nINFO  2025-12-10 18:54:14,372 [shard 0:main] cluster - members_manager.cc:165 - Node configuration updated successfully\r\nINFO  2025-12-10 18:54:14,372 [shard 0:main] main - application.cc:2860 - Started Kafka API server listening at {{:{host: 0.0.0.0, port: 9092}:{nullopt}}}\r\nINFO  2025-12-10 18:54:14,372 [shard 0:main] main - application.cc:2548 - Successfully started Redpanda!\r\nINFO  2025-12-10 18:54:14,374 [shard 0:main] cluster - metrics_reporter"]
[0.000, "o", ".cc:352 - Initialized cluster_id to 73146e35-013c-4558-b73a-17e0eb924737\r\n\r\n==> /var/log/conduktor/console.log <==\r\nOperating System Metrics:\r\n    Provider: cgroupv2\r\n    Effective CPU Count: 8\r\n    CPU Period: 100000us\r\n    CPU Quota: -1\r\n    CPU Shares: -1\r\n    List of Processors: N/A\r\n    List of Effective Processors, 8 total: \r\n    0 1 2 3 4 5 6 7 \r\n    List of Memory Nodes: N/A\r\n    List of Available Memory Nodes, 1 total: \r\n    0 \r\n    Memory Limit: Unlimited\r\n    Memory Soft Limit: 0.00K\r\n    Memory & Swap Limit: Unlimited\r\n"]
[0.000, "o", "    Maximum Processes Limit: Unlimited\r\n"]
[0.000, "o", "\r\n"]
[0.000, "o", "\r\n==> /var/log/conduktor/gateway.log <==\r\n2025-12-10T18:54:14.818+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mBootstrap:38\u001b[m] - Version 3.15.0 b53a0327c9691d4422834ede4ceae0e8173be4ae 2025-11-17T15:07:44+0000\r\n2025-12-10T18:54:14.823+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mBootstrap:57\u001b[m] - Using configuration file: application.yaml\r\n2025-12-10T18:54:15.347+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mProxyConfiguration:300\u001b[m] - No gateway security mode provided. Inferred Security Mode GATEWAY_MANAGED based on Security Protocol SASL_PLAINTEXT.\r\n2025-12-10T18:54:15.404+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mBootstrap:61\u001b[m] - Computed configuration : \r\n---\r\ngatewayClusterId: \"gateway\"\r\ngatewayRackId: null\r\ngatewayGroupId: null\r\nkafkaSelector: !<env>\r\n"]
[0.000, "o", "  prefix: \"KAFKA_\"\r\n"]
[0.000, "o", "hostPortConfiguration:\r\n"]
[0.000, "o", "  bindHost: \"0.0.0.0\"\r\n  advertisedHost: \"localhost\"\r\n  hostPrefix: \"broker\"\r\n  portCount: null\r\n  portStart: 6969\r\n  minBrokerId: 0\r\n  tenantInHostname: false\r\n"]
[0.000, "o", "  advertisedSniPort: null\r\n  sniHostSeparator: \"-\"\r\n  advertisedPorts: null\r\n"]
[0.000, "o", "routing: \"PORT\"\r\n"]
[0.000, "o", "authenticationConfig:\r\n"]
[0.000, "o", "  sslConfig:\r\n    keyStore:\r\n      keyStorePath: null\r\n      keyStorePassword: null\r\n      keyPassword: null\r\n"]
[0.000, "o", "      keyStoreType: \"jks\"\r\n"]
[0.000, "o", "    trustStore:\r\n"]
[0.000, "o", "      trustStorePath: null\r\n      trustStorePassword: null\r\n      trustStoreType: \"jks\"\r\n      clientAuth: \"NONE\"\r\n      sslPrincipalMappingRules: null\r\n    updateContextIntervalMinutes: 5\r\n    enabledProtocols: null\r\n"]
[0.000, "o", "    cipherSuites: null\r\n  connectionsMaxReauthMs: 0\r\n"]
[0.000, "o", "  oauth: null\r\n  authorizationCacheMs: 5000\r\n  mandatoryVCluster: false\r\n  principalResolver: null\r\n  confluentCloud: null\r\n"]
[0.000, "o", "  securityProtocol: \"SASL_PLAINTEXT\"\r\n"]
[0.000, "o", "  securityMode: null\r\n"]
[0.001, "o", "httpConfig:\r\n  port: 8888\r\n  securedMetrics: true\r\n  superUsers:\r\n  - username: \"admin\"\r\n    password: \"***\"\r\n    admin: true\r\n  httpsConfig:\r\n    keyStorePath: null\r\n    keyStorePassword: null\r\n    trustStorePath: null\r\n    truststorePassword: null\r\n    clientAuth: \"NONE\"\r\nthreadConfig:\r\n  downStreamThread: 8\r\n  upstream:\r\n    numberOfThread: 8\r\n    maxPendingTask: 2147483647\r\nmaxIdleProducerMs: 600000\r\nupstreamConnectionConfig:\r\n  numOfConnection: 10\r\n  maxIdleTimeMs: 660000\r\n  connectionPoolType: \"NONE\"\r\nuserPoolConfig:\r\n  jwt:\r\n    secretKey: \"***\"\r\n    signatureAlgorithm: \"HS256\"\r\nconsumerGroupMembershipConfig:\r\n  pollIntervalMs: 3000\r\n  membershipTimeoutMs: 30000\r\n  initMembershipTimeoutMs: 10000\r\naclStoreConfig:\r\n  aclEnabledOnVClusters: false\r\n  allowEveryoneIfNoAclFound: false\r\n  passthroughSuperUsers: null\r\n  aclEnabled: true\r\ninterceptorConfig:\r\n  configLocation: null\r\n  restrictingGlobalInterceptors: null\r\nlicenseConfig:\r\n  keyAlgorithm: \"EC\"\r\n  publicKey: \"MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEMOpF"]
[0.000, "o", "pNv+/ef7vpXFbNUzSVl8KT1ZIyvNrKG8+ZN9zib4XaqAeLADdLW2y3jNIYwnGquknEI30pxcz3/LQhwpGg==\"\r\n  licenseKey: null\r\nfeatureFlags:\r\n  audit: true\r\n  analytics: false\r\n  internalLoadBalancing: true\r\n  blockUnsupportedApis: false\r\n  clusterSimulatorPluginEnabled: false\r\n  topicIdSupport: false\r\ninFlightRequestExpiryMs: 330000\r\nauditLogConfig:\r\n  specVersion: \"0.1.0\"\r\n  topic: null\r\n  numPartitionsOfTopic: -1\r\n  replicationFactorOfTopic: -1\r\n  eventTypes: \"ALL\"\r\n  kafkaPropertiesPrefix: \"GATEWAY_AUDIT_LOG_KAFKA_\"\r\ndataQualityConfig:\r\n  topic: null\r\n  numPartitionsOfTopic: null\r\n  replicationFactorOfTopic: null\r\n  checksCounterIntervalMs: 15000\r\n  retentionHour: 168\r\nanalytics:\r\n  endpoint: \"https://events.eu1.segmentapis.com\"\r\n  writeKey: \"sFyxHHaoDYlnhiFlFmMYu8YWnFczG95Z\"\r\nlogicalOffsetsConfiguration:\r\n  retriesEnabled: false\r\n"]
[0.000, "o", "  retryDelayMs: 10\r\n"]
[0.000, "o", "  maxRetryDelayMs: 100\r\n"]
[0.000, "o", "  delayFactor: 2.0\r\n  maxRetries: 3\r\n"]
[0.000, "o", "gaugeBackendBrokersTimerConfig:\r\n"]
[0.000, "o", "  initialDelayMs: 1000\r\n"]
[0.000, "o", "  delayMs: 60000\r\n"]
[0.000, "o", "storage:\r\n"]
[0.000, "o", "  type: \"KAFKA\"\r\n  kafka:\r\n    defaultKCacheReplicationFactor: -1\r\n  expiryTtlMs: 604800000\r\n  topics:\r\n    topicMappings: null\r\n    interceptorConfigs: null\r\n    acls: null\r\n    consumerOffsets: null\r\n    license: null\r\n    userMappings: null\r\n    encryptionConfigs: null\r\n    groups: null\r\n    encryptionKeys: null\r\n"]
[0.000, "o", "    vclusters: null\r\n"]
[0.000, "o", "shutdownDelayBetweenBrokerMs: 0\r\n"]
[0.000, "o", "autoCreateTopicsEnabled: false\r\n"]
[0.000, "o", "securityProvider: \"DEFAULT\"\r\n\r\n2025-12-10T18:54:15.406+0000 [\u001b[31m      main\u001b[m] [\u001b[32mWARN \u001b[m] [\u001b[34mBootstrap:75\u001b[m] - Current configuration doesn't multiplex connections to Kafka cluster. You can optimize it by setting GATEWAY_UPSTREAM_CONNECTION_POOL_TYPE to ROUND_ROBIN.\r\n"]
[0.000, "o", "\r\n==> /var/log/conduktor/console.log <==\r\n\u001b[35m2025-12-10T18:54:16,403Z\u001b[0;39m \u001b[36m[console:ZScheduler-Worker-0]\u001b[0;39m \u001b[34mINFO \u001b[0;39m i.c.a.license.PlatformLicenseLoader - No license found, fallback to free\r\n\u001b[35m2025-12-10T18:54:16,425Z\u001b[0;39m \u001b[36m[console:ZScheduler-Worker-2]\u001b[0;39m \u001b[34mINFO \u001b[0;39m i.c.d.datasource.DataSourceUtils - Created datasource for config com.zaxxer.hikari.HikariConfig@3198ebfa\r\n\u001b[35m2025-12-10T18:54:16,457Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...\r\n"]
[0.000, "o", "\u001b[35m2025-12-10T18:54:16,490Z\u001b[0;39m \u001b[36m[console:ZScheduler-Worker-0]\u001b[0;39m \u001b[34mINFO \u001b[0;39m i.c.a.license.PlatformLicenseLoader - License Free v3 is unlimited\r\n\u001b[35m2025-12-10T18:54:16,577Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@60c5f836\r\n\u001b[35m2025-12-10T18:54:16,578Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.\r\n\r\n==> /var/log/conduktor/gateway.log <==\r\n2025-12-10T18:54:16.118+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mProxyConfiguration:226\u001b[m] - Using default JRE Security Provider\r\n2025-12-10T18:54:16.150+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mEnvKafkaPropertiesProvider:14\u001b[m] - Loading Kafka Config from environment vars with prefix: KAFKA_\r\n2025-12-10T18:54:16.166+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mClusterConfigurationService:44\u001b[m] - cluster configura"]
[0.000, "o", "tion: \r\n---\r\nbootstrap.servers: \"localhost:9092\"\r\n\r\n"]
[0.842, "o", "\r\n==> /var/log/conduktor/console.log <==\r\n\u001b[35m2025-12-10T18:54:17,053Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m org.flywaydb.core.FlywayExecutor - Database: jdbc:postgresql://localhost:5432/conduktor-console?loginTimeout=30&socketTimeout=120&ssl=false&targetServerType=any (PostgreSQL 16.11)\r\n\u001b[35m2025-12-10T18:54:17,124Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.c.internal.database.base.Schema - Creating schema \"cdk_init\" ...\r\n\u001b[35m2025-12-10T18:54:17,128Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.c.i.s.JdbcTableSchemaHistory - Creating Schema History table \"cdk_init\".\"flyway_schema_history\" ...\r\n\u001b[35m2025-12-10T18:54:17,215Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.core.internal.command.DbMigrate - Current version of schema \"cdk_init\": null\r\n\u001b[35m2025-12-10T18:54:17,226Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.core.internal.command.DbMigrate - Migrat"]
[0.000, "o", "ing schema \"cdk_init\" to version \"1 - initial structure\"\r\n\u001b[35m2025-12-10T18:54:17,248Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.c.i.s.DefaultSqlScriptExecutor - +----------------+\r\n| copy_from_rust |\r\n+----------------+\r\n|                |\r\n+----------------+\r\n\r\n\u001b[35m2025-12-10T18:54:17,259Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.core.internal.command.DbMigrate - Migrating schema \"cdk_init\" to version \"2 - add deployment id column\"\r\n"]
[0.001, "o", "\u001b[35m2025-12-10T18:54:17,278Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[34mINFO \u001b[0;39m o.f.core.internal.command.DbMigrate - Successfully applied 2 migrations to schema \"cdk_init\", now at version v2 (execution time 00:00.032s)\r\n\u001b[35m2025-12-10T18:54:17,489Z\u001b[0;39m \u001b[36m[console:zio-default-blocking-1]\u001b[0;39m \u001b[31mWARN \u001b[0;39m i.c.s.service.SelfInitServiceLive - Nothing in table cdk_init.platform. Let generate secrets \r\n\r\n==> /var/log/conduktor/gateway.log <==\r\n2025-12-10T18:54:17.577+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mPersistentLicenseService:86\u001b[m] - Free license found, valid until: 2025-12-24T18:54:17.303Z\r\n2025-12-10T18:54:17.577+0000 [\u001b[31m      main\u001b[m] [\u001b[32mINFO \u001b[m] [\u001b[34mBootstrap:173\u001b[m] - Starting API server...\r\n\r\n==> /var/log/conduktor/redpanda.log <==\r\nINFO  2025-12-10 18:54:16,859 [shard 0:main] cluster - topics_frontend.cc:142 - Create topics [{configuration: { topic: {kafka/_conduktor_gateway_license}, partition_count: 1, replication_factor: 1, properties: {compressio"]
[0.000, "o", "n: {nullopt}, cleanup_policy_bitflags: {compact}, compaction_strategy: {nullopt}, retention_bytes: {}, retention_duration_ms: {}, segment_size: {nullopt}, timestamp_type: {nullopt}, recovery_enabled: {nullopt}, shadow_indexing: {disabled}, read_replica: {nullopt}, read_replica_bucket: {nullopt} remote_topic_properties: {nullopt}, batch_max_bytes: {nullopt}, retention_local_target_bytes: {}, retention_local_target_ms: {}, remote_delete: true, segment_ms: {}, record_key_schema_id_validation: {nullopt}, record_key_schema_id_validation_compat: {nullopt}, record_key_subject_name_strategy:  {nullopt}, record_key_subject_name_strategy_compat: {nullopt}, record_value_schema_id_validation: {nullopt},  record_value_schema_id_validation_compat: {nullopt}, record_value_subject_name_strategy: {nullopt}, record_value_subject_name_strategy_compat: {nullopt}, initial_retention_local_target_bytes: {}, initial_retention_local_target_ms: {}, mpx_virtual_cluster_id: {nullopt}, write_caching: {nullopt}, flush_ms: {nullopt}, flush"]
[0.000, "o", "_bytes: {nullopt}}}, custom_assignments: {}},]\r\nINFO  2025-12-10 18:54:16,903 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] consensus.cc:1418 - Starting with voted_for {id: -2147483648, revision: -9223372036854775808} term 0 initial_state true\r\nINFO  2025-12-10 18:54:16,925 [shard 0:main] offset_translator - ntp: {kafka/_conduktor_gateway_license/0} - offset_translator.cc:120 - resetting offset translation state\r\nINFO  2025-12-10 18:54:16,947 [shard 0:main] offset_translator - ntp: {kafka/_conduktor_gateway_license/0} - offset_translator.cc:166 - started, state: {base offset/delta: -9223372036854775808/0, map size: 1, last delta: 0}, highest_known_offset: -9223372036854775808\r\nINFO  2025-12-10 18:54:16,947 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] consensus.cc:1462 - Current log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_"]
[0.000, "o", "term:-9223372036854775808}, read bootstrap state: data_seen 0 config_seen 0 eol false commit 0 term 0 prev_idx 0 prev_term 0 config_tracker -9223372036854775808 commit_base_tracker -9223372036854775808 configurations {}\r\nINFO  2025-12-10 18:54:16,947 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] consensus.cc:1484 - Truncating configurations at -9223372036854775808\r\nINFO  2025-12-10 18:54:16,969 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] consensus.cc:927 - starting pre-vote leader election, current term: 0, leadership transfer: false\r\nINFO  2025-12-10 18:54:16,992 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] consensus.cc:1587 - started raft, log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, term: 0, configuration: {current: {voters: {{id: 0, revision: 8}}, learners: {}}, "]
[0.000, "o", "old:{nullopt}, revision: 8, update: {nullopt}, version: 5}}\r\nINFO  2025-12-10 18:54:17,004 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] vote_stm.cc:418 - becoming the leader term:1\r\nINFO  2025-12-10 18:54:17,005 [shard 0:main] storage - segment.cc:806 - Creating new segment /var/lib/conduktor/redpanda/kafka/_conduktor_gateway_license/0_8/0-1-v1.log\r\nINFO  2025-12-10 18:54:17,030 [shard 0:main] tx - [{kafka/_conduktor_gateway_license/0}] - rm_stm.cc:298 - Setting bootstrap committed offset to: 0\r\nINFO  2025-12-10 18:54:17,030 [shard 0:main] raft - [group_id:1, {kafka/_conduktor_gateway_license/0}] vote_stm.cc:433 - became the leader term: 1\r\nWARN  2025-12-10 18:54:17,225 [shard 0:main] cluster - id_allocator_frontend.cc:270 - can't find {kafka_internal/id_allocator} in the metadata cache\r\nINFO  2025-12-10 18:54:17,225 [shard 0:main] cluster - topics_frontend.cc:142 - Create topics [{configuration: { topic: {kafka_internal/id_allocator}, partition_count: 1, replication_factor: 1, pro"]
[0.000, "o", "perties: {compression: {nullopt}, cleanup_policy_bitflags: {none}, compaction_strategy: {nullopt}, retention_bytes: {}, retention_duration_ms: {}, segment_size: {nullopt}, timestamp_type: {nullopt}, recovery_enabled: {nullopt}, shadow_indexing: {disabled}, read_replica: {nullopt}, read_replica_bucket: {nullopt} remote_topic_properties: {nullopt}, batch_max_bytes: {nullopt}, retention_local_target_bytes: {}, retention_local_target_ms: {}, remote_delete: true, segment_ms: {}, record_key_schema_id_validation: {nullopt}, record_key_schema_id_validation_compat: {nullopt}, record_key_subject_name_strategy:  {nullopt}, record_key_subject_name_strategy_compat: {nullopt}, record_value_schema_id_validation: {nullopt},  record_value_schema_id_validation_compat: {nullopt}, record_value_subject_name_strategy: {nullopt}, record_value_subject_name_strategy_compat: {nullopt}, initial_retention_local_target_bytes: {}, initial_retention_local_target_ms: {}, mpx_virtual_cluster_id: {nullopt}, write_caching: {nullopt}, flush_ms:"]
[0.000, "o", " {nullopt}, flush_bytes: {nullopt}}}, custom_assignments: {}},]\r\nINFO  2025-12-10 18:54:17,253 [shard 0:main] raft - [group_id:2, {kafka_internal/id_allocator/0}] consensus.cc:1418 - Starting with voted_for {id: -2147483648, revision: -9223372036854775808} term 0 initial_state true\r\nINFO  2025-12-10 18:54:17,275 [shard 0:main] raft - [group_id:2, {kafka_internal/id_allocator/0}] consensus.cc:1462 - Current log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, read bootstrap state: data_seen 0 config_seen 0 eol false commit 0 term 0 prev_idx 0 prev_term 0 config_tracker -9223372036854775808 commit_base_tracker -9223372036854775808 configurations {}\r\nINFO  2025-12-10 18:54:17,275 [shard 0:main] raft - [group_id:2, {kafka_internal/id_allocator/0}] consensus.cc:1484 - Truncating configurations at -9223372036854775808\r\nINFO  2025-12-10 18:54:17,298 [shard 0:main]"]
[0.000, "o", " raft - [group_id:2, {kafka_internal/id_allocator/0}] consensus.cc:927 - starting pre-vote leader election, current term: 0, leadership transfer: false\r\nINFO  2025-12-10 18:54:17,309 [shard 0:main] raft - [group_id:2, {kafka_internal/id_allocator/0}] consensus.cc:1587 - started raft, log offsets: {start_offset:-9223372036854775808, committed_offset:-9223372036854775808, committed_offset_term:-9223372036854775808, dirty_offset:-9223372036854775808, dirty_offset_term:-9223372036854775808}, term: 0, configuration: {current: {voters: {{id: 0, revision: 9}}, learners: {}}, old:{nullopt}, revision: 9, update: {nullopt}, version: 5}}\r\nINFO  2025-12-10 18:54:17,321 [shard 0:main] raft - [group_id:2, {kafka_internal/id_allocator/0}] vote_stm.cc:418 - becoming the leader term:1\r\nINFO  2025-12-10 18:54:17,321 [shard 0:main] storage - segment.cc:806 - Creating new segment /var/lib/conduktor/redpanda/kafka_internal/id_allocator/0_9/0-1-v1.log\r\nINFO  2025-12-10 18:54:17,329 [shard 0:main] cluster - partition_balancer_plann"]
[0.000, "o", "er.cc:1972 - counts rebalancing objective in domain -1: 1 -> 1\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:17,329 [shard 0:main] cluster - partition_balancer_planner.cc:1972 - counts rebalancing objective in domain 0: 1 -> 1\r\nINFO  2025-12-10 18:54:17,329 [shard 0:main] cluster - partition_balancer_backend.cc:423 - last status: in_progress; violations: unavailable nodes: 0, full nodes: 0; nodes to rebalance count: 1; on demand rebalance requested: false; updates in progress: 0; action counts: reassignments: 0, cancellations: 0, failed: 0; counts rebalancing finished: true, force refresh health report: false\r\n"]
[0.000, "o", "INFO  2025-12-10 18:54:17,329 [shard 0:main] cluster - members_manager.cc:376 - applying finish_reallocations_cmd, offset: 10, node id: 0\r\nINFO  2025-12-10 18:54:17,346 [shard 0:main] raft - [group_id:2, {kafka_internal/id_allocator/0}] vote_stm.cc:433 - became the leader term: 1\r\n"]
[0.403, "o", "^C"]
[0.008, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007\u001b]1;..gle_container\u0007"]
[0.002, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.045, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b="]
[0.000, "o", "\u001b[?2004h"]
[0.563, "o", "."]
[0.180, "o", "\b./"]
[0.007, "o", "."]
[0.226, "o", "s"]
[0.249, "o", "\b \b"]
[0.120, "o", "\b \b"]
[0.270, "o", "s"]
[0.192, "o", "e"]
[0.117, "o", "t"]
[0.026, "o", "up_gateway.sh\u001b[1m \u001b[0m"]
[0.421, "o", "\b\u001b[0m \b"]
[0.000, "o", "\u001b[?1l\u001b>"]
[0.000, "o", "\u001b[?2004l\r\r\n"]
[0.001, "o", "\u001b]2;./setup_gateway.sh\u0007\u001b]1;./setup_gateway.sh\u0007"]
[0.079, "o", "  Attempt 1/90 - Conduktor Console not ready yet...\r\n"]
[5.036, "o", "  Attempt 2/90 - Conduktor Console not ready yet...\r\n"]
[5.073, "o", "Conduktor Console is ready\r\n"]
[0.016, "o", "Gateway Admin API is ready\r\n"]
[0.000, "o", "\r\nSetting up vClusters: demo, demo-acl\r\n"]
[0.000, "o", "\r\nCreating vCluster: demo...\r\n"]
[1.394, "o", "VirtualCluster/demo: Created\r\n"]
[0.020, "o", "GatewayServiceAccount/admin: Created\r\n"]
[0.044, "o", "\r\nAdding vCluster: demo... to Console\r\n"]
[0.828, "o", "KafkaCluster/demo: Created\r\n"]
[0.005, "o", "\r\nCreating vCluster: demo-acl (ACL enabled)...\r\n"]
[0.823, "o", "VirtualCluster/demo-acl: Created\r\n"]
[0.044, "o", "GatewayServiceAccount/admin: Created\r\n"]
[0.030, "o", "GatewayServiceAccount/user1: Created\r\n"]
[0.110, "o", "\r\nAdding vCluster: demo-acl to console\r\n"]
[1.159, "o", "KafkaCluster/demo-acl: Created\r\n"]
[0.589, "o", "ServiceAccount/user1: Created\r\n"]
[0.010, "o", "\r\n=== Virtual ACL Demo ===\r\n\r\n"]
[0.000, "o", "Admin can create the topic\r\n"]
[1.741, "o", "Created topic click-stream.\r\n"]
[0.366, "o", "\r\nAdmin can write in the topic\r\n"]
[1.257, "o", "\r\n"]
[0.000, "o", "Admin can read from the topic\r\n"]
[3.948, "o", "{\"event\":\"admin-test\"}\r\n"]
[1.009, "o", "Processed a total of 1 messages\r\n"]
[0.358, "o", "\r\n"]
[0.000, "o", "user1 cannot write to the topic\r\n"]
[0.959, "o", "[2025-12-10 22:54:45,327] ERROR [Producer clientId=console-producer] Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender)\r\norg.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed.\r\n"]
[0.001, "o", "[2025-12-10 22:54:45,328] ERROR Error when sending message to topic click-stream with key: null, value: 22 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)\r\norg.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed.\r\n"]
[0.361, "o", "\r\n"]
[0.000, "o", "user1 can read from the topic, as kafka-console-consumer is using a consumer group, we specify its name to match the ACL\r\n"]
[3.939, "o", "{\"event\":\"admin-test\"}\r\n"]
[1.023, "o", "Processed a total of 1 messages\r\n"]
[0.372, "o", "\r\n==============================================\r\nSetup completed!\r\n==============================================\r\n\r\nConsole: http://localhost:8080\r\n    Username: admin@demo.dev\r\n    Password: 123_ABC_abc\r\n\r\nGateway API: http://localhost:8888\r\n    Username: admin\r\n    Password: conduktor\r\n\r\nvCluster: demo (ACL disabled)\r\n  Username: admin\r\n  Password: eyJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwidmNsdXN0ZXIiOiJkZW1vIiwiZXhwIjoxNzczMTY4ODcyfQ._UxVJFzt33gurH_rNW3g3-2mKWKY0FTXG2cfH_fKcco\r\n\r\nvCluster: demo-acl (ACL enabled)\r\n  admin:\r\n    Password: eyJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwidmNsdXN0ZXIiOiJkZW1vLWFjbCIsImV4cCI6MTc3MzE2ODg3M30.4jBkBlGRe7UpmYGTDOy2vJxdUvSH304KhBJszmcg4gk\r\n  user1 (can only access click.* topics):\r\n    Password: eyJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6InVzZXIxIiwidmNsdXN0ZXIiOiJkZW1vLWFjbCIsImV4cCI6MTc3MzE2ODg3M30.VVN0L0dKnr4akTfC6JSIeh1t6BKVx7sgGn8uZyuGoNc\r\n\r\n==============================================\r\n"]
[0.000, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.001, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007"]
[0.000, "o", "\u001b]1;..gle_container\u0007"]
[0.002, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.052, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b="]
[0.000, "o", "\u001b[?2004h"]
[1.556, "o", "c"]
[0.064, "o", "\bco"]
[0.077, "o", "n"]
[0.656, "o", "d"]
[0.094, "o", "u"]
[0.171, "o", "j"]
[0.111, "o", "t"]
[0.358, "o", "\b \b"]
[0.133, "o", "\b \b"]
[0.172, "o", "k"]
[0.316, "o", "t"]
[0.120, "o", "o"]
[0.093, "o", "r"]
[0.114, "o", " "]
[0.210, "o", "g"]
[0.081, "o", "e"]
[0.124, "o", "t"]
[0.027, "o", " "]
[0.156, "o", "a"]
[0.062, "o", "l"]
[0.124, "o", "l"]
[0.347, "o", "\u001b[?1l\u001b>"]
[0.000, "o", "\u001b[?2004l\r\r\n"]
[0.001, "o", "\u001b]2;conduktor get all\u0007\u001b]1;conduktor\u0007"]
[1.033, "o", "---\r\n"]
[0.001, "o", "apiVersion: v1\r\nkind: DataQualityRule\r\n"]
[0.000, "o", "metadata:\r\n    name: enforce_avro\r\n    createdAt: \"2025-12-10T18:54:19.815103Z\"\r\n    updatedAt: \"2025-12-10T18:54:19.815103Z\"\r\n    createdBy: admin\r\n    updatedBy: admin\r\n    attachedPolicies: []\r\n    builtIn: true\r\n"]
[0.000, "o", "spec:\r\n    displayName: Enforce Avro\r\n    description: Ensures that Kafka messages have an Avro schema registered in a Schema Registry\r\n    customErrorMessage: Message is not Avro-encoded\r\n    type: EnforceAvro\r\n---\r\n"]
[0.000, "o", "apiVersion: v1\r\nkind: DataQualityRule\r\n"]
[0.000, "o", "metadata:\r\n    name: enforce_schema_id\r\n    createdAt: \"2025-12-10T18:54:20.243903Z\"\r\n    updatedAt: \"2025-12-10T18:54:20.243903Z\"\r\n    createdBy: admin\r\n    updatedBy: admin\r\n    attachedPolicies: []\r\n    builtIn: true\r\n"]
[0.000, "o", "spec:\r\n    displayName: Enforce schema ID\r\n    description: Ensures that Kafka messages start with a magic byte and a schemaId without calling the schema registry\r\n    customErrorMessage: Message is missing a valid schema ID\r\n    type: EnforceSchemaId\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: GatewayServiceAccount\r\n"]
[0.000, "o", "metadata:\r\n    name: admin\r\n    vCluster: demo\r\n"]
[0.000, "o", "spec:\r\n    type: LOCAL\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: GatewayServiceAccount\r\n"]
[0.000, "o", "metadata:\r\n    name: admin\r\n    vCluster: demo-acl\r\n"]
[0.000, "o", "spec:\r\n    type: LOCAL\r\n---\r\n"]
[0.001, "o", "apiVersion: gateway/v2\r\nkind: GatewayServiceAccount\r\n"]
[0.000, "o", "metadata:\r\n    name: user1\r\n    vCluster: demo-acl\r\n"]
[0.000, "o", "spec:\r\n    type: LOCAL\r\n---\r\n"]
[0.000, "o", "apiVersion: v2\r\nkind: Group\r\n"]
[0.000, "o", "metadata:\r\n    name: admin\r\n"]
[0.000, "o", "spec:\r\n    displayName: admin\r\n    description: Built-in group with admin level access\r\n    members:\r\n        - admin@demo.dev\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: Interceptor\r\n"]
[0.000, "o", "metadata:\r\n    name: encrypt-full-message-on-produce\r\n    scope:\r\n        vCluster: passthrough\r\n        group: null\r\n        username: null\r\n"]
[0.000, "o", "spec:\r\n    comment: Encrypt the payload using an in-memory kms (do not use in production)\r\n    pluginClass: io.conduktor.gateway.interceptor.EncryptPlugin\r\n    priority: 100\r\n    config:\r\n        topic: .*_encrypted$\r\n        payload:\r\n            keySecretId: in-memory-kms://myKeySecretId\r\n            algorithm:\r\n                type: AES128_GCM\r\n                kms: IN_MEMORY\r\n---\r\n"]
[0.001, "o", "apiVersion: gateway/v2\r\nkind: Interceptor\r\n"]
[0.000, "o", "metadata:\r\n    name: guard-create-project-topics\r\n    scope:\r\n        vCluster: passthrough\r\n        group: null\r\n        username: null\r\n"]
[0.000, "o", "spec:\r\n    comment: Make sure we do not overuse partitions\r\n    pluginClass: io.conduktor.gateway.interceptor.safeguard.CreateTopicPolicyPlugin\r\n    priority: 100\r\n    config:\r\n        topic: project-.*\r\n        numPartition:\r\n            min: 1\r\n            max: 3\r\n            action: BLOCK\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: Interceptor\r\n"]
[0.000, "o", "metadata:\r\n    name: decrypt-full-message-on-consume\r\n    scope:\r\n        vCluster: passthrough\r\n        group: null\r\n        username: null\r\n"]
[0.000, "o", "spec:\r\n    comment: Decrypt\r\n    pluginClass: io.conduktor.gateway.interceptor.DecryptPlugin\r\n    priority: 100\r\n    config:\r\n        topic: .*_encrypted$\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: Interceptor\r\n"]
[0.000, "o", "metadata:\r\n    name: guard-produce-policy\r\n    scope:\r\n        vCluster: passthrough\r\n        group: null\r\n        username: null\r\n"]
[0.001, "o", "spec:\r\n    comment: Prevent data loss and require compression\r\n    pluginClass: io.conduktor.gateway.interceptor.safeguard.ProducePolicyPlugin\r\n    priority: 100\r\n    config:\r\n        acks:\r\n            value:\r\n                - -1\r\n            action: BLOCK\r\n        compressions:\r\n            value:\r\n                - GZIP\r\n                - LZ4\r\n                - ZSTD\r\n                - SNAPPY\r\n            action: INFO\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: Interceptor\r\n"]
[0.000, "o", "metadata:\r\n    name: mask-sensitive-fields\r\n    scope:\r\n        vCluster: passthrough\r\n        group: null\r\n        username: null\r\n"]
[0.000, "o", "spec:\r\n    comment: Mask sensitive data\r\n    pluginClass: io.conduktor.gateway.interceptor.FieldLevelDataMaskingPlugin\r\n    priority: 100\r\n    config:\r\n        topic: ^[A-Za-z]*_masked$\r\n        schemaRegistryConfig:\r\n            host: http://redpanda-0:8081\r\n        policies:\r\n            - name: Mask credit card\r\n              rule:\r\n                type: MASK_ALL\r\n              fields:\r\n                - profile.creditCardNumber\r\n                - contact.email\r\n            - name: Partial mask phone\r\n              rule:\r\n                type: MASK_FIRST_N\r\n                maskingChar: '*'\r\n                numberOfChars: 9\r\n              fields:\r\n                - contact.phone\r\n---\r\n"]
[0.001, "o", "apiVersion: v2\r\nkind: KafkaCluster\r\n"]
[0.000, "o", "metadata:\r\n    name: demo-acl\r\n"]
[0.000, "o", "spec:\r\n    displayName: demo-acl (ACL enabled)\r\n    bootstrapServers: localhost:6969\r\n    properties:\r\n        security.protocol: SASL_PLAINTEXT\r\n        sasl.mechanism: PLAIN\r\n        sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username='admin' password='eyJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwidmNsdXN0ZXIiOiJkZW1vLWFjbCIsImV4cCI6MTc3MzE2ODg3M30.4jBkBlGRe7UpmYGTDOy2vJxdUvSH304KhBJszmcg4gk';\r\n    kafkaFlavor:\r\n        url: http://localhost:8888\r\n        user: admin\r\n        password: conduktor\r\n        virtualCluster: demo-acl\r\n        ignoreUntrustedCertificate: false\r\n        type: Gateway\r\n---\r\n"]
[0.000, "o", "apiVersion: v2\r\nkind: KafkaCluster\r\n"]
[0.000, "o", "metadata:\r\n    name: demo\r\n"]
[0.000, "o", "spec:\r\n    displayName: demo\r\n    bootstrapServers: localhost:6969\r\n    properties:\r\n        security.protocol: SASL_PLAINTEXT\r\n        sasl.mechanism: PLAIN\r\n        sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username='admin' password='eyJhbGciOiJIUzI1NiJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwidmNsdXN0ZXIiOiJkZW1vIiwiZXhwIjoxNzczMTY4ODcyfQ._UxVJFzt33gurH_rNW3g3-2mKWKY0FTXG2cfH_fKcco';\r\n    kafkaFlavor:\r\n        url: http://localhost:8888\r\n        user: admin\r\n        password: conduktor\r\n        virtualCluster: demo\r\n        ignoreUntrustedCertificate: false\r\n        type: Gateway\r\n---\r\n"]
[0.001, "o", "apiVersion: v2\r\nkind: KafkaCluster\r\n"]
[0.000, "o", "metadata:\r\n    name: local-kafka\r\n"]
[0.000, "o", "spec:\r\n    displayName: local-kafka\r\n    bootstrapServers: localhost:9092\r\n    color: '#6A57C8'\r\n    icon: kafka\r\n    schemaRegistry:\r\n        url: http://localhost:8081\r\n        security:\r\n            type: NoSecurity\r\n        ignoreUntrustedCertificate: false\r\n        type: ConfluentLike\r\n---\r\n"]
[0.000, "o", "apiVersion: v2\r\nkind: User\r\n"]
[0.000, "o", "metadata:\r\n    name: admin@demo.dev\r\n    lastLoginDate: \"2025-12-10T18:54:56.644540Z\"\r\nspec: {}\r\n---\r\n"]
[0.000, "o", "apiVersion: gateway/v2\r\nkind: VirtualCluster\r\n"]
[0.000, "o", "metadata:\r\n    name: demo\r\n"]
[0.000, "o", "spec:\r\n    aclEnabled: false\r\n    aclMode: KAFKA_API\r\n    superUsers:\r\n        - admin\r\n    type: Standard\r\n    bootstrapServers: localhost:6969\r\n    clientProperties:\r\n        PLAIN:\r\n            security.protocol: SASL_PLAINTEXT\r\n            sasl.mechanism: PLAIN\r\n            sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username='{{username}}' password='{{password}}';\r\n---\r\n"]
[0.001, "o", "apiVersion: gateway/v2\r\nkind: VirtualCluster\r\n"]
[0.000, "o", "metadata:\r\n    name: demo-acl\r\n"]
[0.000, "o", "spec:\r\n    aclEnabled: true\r\n    aclMode: KAFKA_API\r\n    superUsers:\r\n        - admin\r\n    type: Standard\r\n    bootstrapServers: localhost:6969\r\n    clientProperties:\r\n        PLAIN:\r\n            security.protocol: SASL_PLAINTEXT\r\n            sasl.mechanism: PLAIN\r\n            sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username='{{username}}' password='{{password}}';\r\n"]
[0.002, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007"]
[0.000, "o", "\u001b]1;..gle_container\u0007"]
[0.003, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.040, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[3.539, "o", "conduktor get all"]
[0.269, "o", " "]
[0.165, "o", "|"]
[0.170, "o", " "]
[3.879, "o", "\u001b[?2004l\r\r\n"]
[0.000, "o", "\u001b[1m\u001b[7m%\u001b[27m\u001b[1m\u001b[0m                                                                                                                     \r \r"]
[0.000, "o", "\u001b]2;framiere@m1-florent:~/rd/conduktor_quick_start_in_a_single_container\u0007"]
[0.000, "o", "\u001b]1;..gle_container\u0007"]
[0.003, "o", "\u001b]7;file://m1-florent.local/Users/framiere/rd/conduktor_quick_start_in_a_single_container\u001b\\"]
[0.058, "o", "\r\u001b[0m\u001b[27m\u001b[24m\u001b[J\u001b[35m\u001b[00m ~/rd/conduktor_quick_start_in_a_single_container/ \u001b[00m\u001b[35m[\u001b[32mmain\u001b[35m]\u001b[39m \u001b[00m\u001b[K"]
[0.000, "o", "\u001b[?1h\u001b=\u001b[?2004h"]
[0.278, "o", "\u001b[?2004l\r\r\n"]
[0.004, "x", "130"]
